{"cells":[{"cell_type":"code","execution_count":5,"id":"43dece4f","metadata":{"id":"43dece4f","executionInfo":{"status":"ok","timestamp":1753473242794,"user_tz":-540,"elapsed":5,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"outputs":[],"source":["!pip install face_alignment"]},{"cell_type":"code","execution_count":12,"id":"d2acfc52","metadata":{"id":"d2acfc52","executionInfo":{"status":"ok","timestamp":1753465260804,"user_tz":-540,"elapsed":10,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"outputs":[],"source":["import cv2\n","import torch\n","import torchvision.transforms as T\n","import numpy as np\n","import torch.nn.functional as F\n","from torchvision import models\n","from torch import nn\n","import os\n","import glob\n","import pandas as pd\n","import face_alignment\n","from tqdm import tqdm\n","from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n","import timm"]},{"cell_type":"code","execution_count":4,"id":"efded1e8","metadata":{"id":"efded1e8","executionInfo":{"status":"ok","timestamp":1753473234661,"user_tz":-540,"elapsed":6,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"outputs":[],"source":["# ‚úÖ Set the device to MPS(for Mac) if available, otherwise fallback to CUDA or CPU\n","device = torch.device(\"mps\") if torch.backends.mps.is_available() else (\n","torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","source":["# Mount Google Drive to access files in Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"PKaSnDi1LDM1","executionInfo":{"status":"ok","timestamp":1753473228433,"user_tz":-540,"elapsed":9,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"id":"PKaSnDi1LDM1","execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":13,"id":"4cb7bf3b","metadata":{"id":"4cb7bf3b","executionInfo":{"status":"ok","timestamp":1753465264396,"user_tz":-540,"elapsed":4,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"outputs":[],"source":["# Model Structure\n","class Model(nn.Module):\n","    def __init__(self, num_binary_classes=2, num_method_classes=7,model_name=\"resnext50_32x4d\", lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n","        super(Model, self).__init__()\n","        self.model_name = model_name\n","\n","        # Select the model for comparison (ResNeXt, Xception, or EfficientNet)\n","        if self.model_name==\"resnext50_32x4d\":\n","          model = models.resnext50_32x4d(pretrained = True) # Using ResNeXt50-32x4d\n","          self.model = nn.Sequential(*list(model.children())[:-2])\n","          self.latent_dim = 2048\n","        elif self.model_name==\"xception\":\n","          self.latent_dim = 2048\n","          model = timm.create_model('xception', pretrained=True, features_only=False) # Using Xception\n","          self.model = nn.Sequential(*list(model.children())[:-2])\n","        elif self.model_name==\"EfficientNet-b0\":\n","           self.latent_dim = 1280\n","           weights = EfficientNet_B0_Weights.DEFAULT  # Using EfficientNet B0\n","           model = efficientnet_b0(weights=weights)\n","           self.model = nn.Sequential(*list(model.features))\n","\n","        print(\"latet_dim: \",self.latent_dim)\n","\n","        self.lstm = nn.LSTM(self.latent_dim,hidden_dim, lstm_layers,  bidirectional)\n","        self.relu = nn.LeakyReLU()\n","        self.dp = nn.Dropout(0.5)\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","\n","        # Two outputs: binary classification and method classification\n","        self.binary_classifier = nn.Linear(hidden_dim, num_binary_classes)\n","        self.method_classifier = nn.Linear(hidden_dim, num_method_classes)\n","\n","    def forward(self, x):\n","        batch_size,seq_length, c, h, w = x.shape\n","        x = x.view(batch_size * seq_length, c, h, w)\n","        fmap = self.model(x)\n","        x = self.avgpool(fmap)\n","        x = x.view(batch_size,seq_length,self.latent_dim) # resnext50_32x4d, xception : 2048, efficientnet-b0 : 1280\n","        x_lstm,_ = self.lstm(x,None)\n","        pooled = torch.mean(x_lstm, dim=1)\n","        return fmap, self.binary_classifier(self.dp(pooled)), self.method_classifier(self.dp(pooled))"]},{"cell_type":"markdown","source":["## Grad-cam"],"metadata":{"id":"hrkCIPM9JF8a"},"id":"hrkCIPM9JF8a"},{"cell_type":"code","source":["# ‚úÖ Grad-CAM computation for binary classification\n","def compute_gradcam_binary(model, input_tensor, target_class=0):\n","    fmap = None\n","    grad = None\n","\n","    def fw_hook(module, inp, out):\n","        nonlocal fmap\n","        fmap = out.detach()\n","\n","    def bw_hook(module, grad_in, grad_out):\n","        nonlocal grad\n","        grad = grad_out[0].detach()\n","\n","    last_layer = model.model[-1]\n","    f = last_layer.register_forward_hook(fw_hook)\n","    b = last_layer.register_backward_hook(bw_hook)\n","\n","    input_tensor = input_tensor.to(device).unsqueeze(0).unsqueeze(0).requires_grad_(True)\n","    _, binary_output, method_output = model(input_tensor)\n","\n","    # Get the probability of the target class\n","    prob = F.softmax(binary_output, dim=1)[0, target_class].item()\n","\n","    # Predict binary and method classes\n","    binary_pred = torch.argmax(binary_output, dim=1).item()   # 0: fake, 1: real\n","    method_pred = torch.argmax(method_output, dim=1).item()   # 0: original, 1~6: fake methods, 7: others\n","\n","    # üî¥ Condition 1: If the prediction is real and the method is original, skip CAM computation / Ï°∞Í±¥ 1: real(1) + original(0) ‚Üí CAM X\n","    if binary_pred == 1 and method_pred==0:\n","        cam = np.zeros((input_tensor.shape[-2], input_tensor.shape[-1]))\n","        f.remove()\n","        b.remove()\n","        return cam,prob\n","\n","    # Grad-CAM for fake class (target_class = 0)\n","    target_class = 0\n","    model.zero_grad()\n","    binary_output[0, target_class].backward()\n","\n","    # Compute Grad-CAM\n","    weights = grad.mean(dim=[2, 3], keepdim=True)\n","    cam = (weights * fmap).sum(dim=1, keepdim=True)\n","    cam = F.relu(cam).squeeze().cpu().numpy()\n","\n","    # üîµ Condition 2: If the prediction is fake and the method is not original, enhance CAM / Ï°∞Í±¥ 2: fake (0) + method (1~7) (‚â† 0) ‚Üí CAM ‚Üë\n","    if binary_pred == 0 and method_pred != 0:\n","        cam *=1.5\n","\n","    # Normalize and resize the CAM\n","    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n","    cam = cv2.resize(cam, (input_tensor.shape[-1], input_tensor.shape[-2]))\n","\n","\n","\n","    f.remove()\n","    b.remove()\n","    return cam, prob"],"metadata":{"id":"i8IRfawwOCWV","executionInfo":{"status":"ok","timestamp":1753465266949,"user_tz":-540,"elapsed":15,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"id":"i8IRfawwOCWV","execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"id":"8ea7f3e5","metadata":{"id":"8ea7f3e5","executionInfo":{"status":"ok","timestamp":1753465269892,"user_tz":-540,"elapsed":9,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"outputs":[],"source":["# ‚úÖ Function to process video, compute Grad-CAM, and save frames\n","def process_video_and_save_frames(input_video_path, output_video_path, model, frame_dir):\n","\n","    # Check the device (MPS, CUDA, or CPU)\n","    device = torch.device(\"mps\") if torch.backends.mps.is_available() else (\n","    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    )\n","    print(f\"Using device: {device}\")\n","\n","    # Check how many videos are already processed and saved\n","    input_path = f'{input_video_path}/*.mp4'\n","    video_files = glob.glob(input_path)\n","    print(len(video_files))\n","    already_present_count = glob.glob(output_video_path+ '/*.mp4')\n","    print(\"No of videos already present \", len(already_present_count))\n","\n","    # Create output folders if not exist\n","    os.makedirs(frame_dir, exist_ok=True)\n","    os.makedirs(output_video_path, exist_ok=True)\n","    top_jpg_dir = os.path.join(output_video_path, \"Top_jpg\")\n","    os.makedirs(top_jpg_dir, exist_ok=True)\n","\n","\n","    # Load REAL/FAKE labels\n","    df = pd.read_excel(predictions_file_path)\n","    for video_file in tqdm(video_files):\n","        file_name=os.path.basename(video_file)\n","        result = str(df[df['Filename'] == file_name]['label'].iloc[0])[0] + str(df[df['Filename'] == file_name]['Prediction'].iloc[0])[0]+str(df[df['Filename'] == file_name]['Predicted_method'].iloc[0])\n","\n","        f_name=f'({result})_'+file_name\n","        out_path = os.path.join(output_video_path,f_name) # Extract output video file name\n","        file_exists = glob.glob(out_path + \"*\")\n","\n","        if(len(file_exists) != 0): # Skip if video already exists\n","            print(\"File Already exists: \" , out_path)\n","            continue\n","\n","        cap = cv2.VideoCapture(video_file)\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","        # Set output MP4 file name\n","        name=os.path.splitext(file_name)[0]\n","        out = cv2.VideoWriter(out_path,cv2.VideoWriter_fourcc('M','J','P','G'), fps, (w, h))\n","\n","        # Preprocessing transforms\n","        transform = T.Compose([\n","            T.ToPILImage(),\n","            T.Resize((224, 224)),\n","            T.ToTensor(),\n","            T.Normalize([0.485, 0.456, 0.406],\n","                                [0.229, 0.224, 0.225])\n","        ])\n","\n","        # Load face_alignment library for face detection\n","        fa = face_alignment.FaceAlignment(\n","            face_alignment.LandmarksType.TWO_D,\n","            device=str(device)\n","        )\n","\n","        frame_count = 0\n","\n","        frame_scores = []\n","        frame_images = []\n","        roi_result=[]\n","\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            original = frame.copy()\n","            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            img = transform(img).to(device)\n","\n","            # 1Ô∏è‚É£ Compute Grad-CAM for binary classification\n","            cam , score= compute_gradcam_binary(model, img)\n","\n","            # cam_activation = float(np.mean(cam))  # the mean of CAM Activation\n","\n","            # Generate Grad-CAM overlay for the frame\n","            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n","            heatmap = cv2.resize(heatmap, (original.shape[1], original.shape[0]))\n","            overlay = 0.4 * heatmap + 0.6 * original\n","            overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n","\n","            frame_scores.append((frame_count, score))\n","            frame_images.append((frame_count, overlay))\n","\n","            out.write(overlay) # Save Grad-CAM overlay video\n","\n","            # Save the Grad-CAM overlay frame as an image\n","            frame_path = os.path.join(frame_dir, f\"({result})_{name}_{frame_count:04d}_{score:10f}.jpg\")\n","            cv2.imwrite(frame_path, overlay)\n","            frame_count += 1\n","\n","        # ‚úÖ Save top 10 frames with highest scores(probability)\n","        frame_scores.sort(key=lambda x: x[1], reverse=True)\n","        top_10_indices = [idx for idx, score in frame_scores[:10]]\n","\n","        for rank, idx in enumerate(top_10_indices):\n","            img = frame_images[idx][1]\n","            score = frame_scores[rank][1]\n","\n","            top_frame_path = os.path.join(top_jpg_dir, f\"{name}_TOP{rank + 1}_Score{score:10f}.jpg\")\n","            cv2.imwrite(top_frame_path, img)   # Save top 10 frames\n","\n","\n","        cap.release()  # Save frames\n","        out.release()  # Save videos\n","\n","        print(f\"‚úÖ Grad-CAM video saved: {output_video_path}\")\n","        print(f\"‚úÖ {frame_count} frame images saved: {frame_dir}/frame_XXXX.jpg\")\n"]},{"cell_type":"code","execution_count":1,"id":"2bfb49c6","metadata":{"id":"2bfb49c6","executionInfo":{"status":"ok","timestamp":1753473201410,"user_tz":-540,"elapsed":17,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"outputs":[],"source":["# Define checkpoint and model selection\n","checkpoint_name=\"checkpoint_v35\"\n","selected_model=\"EfficientNet-b0\"\n","\n","\n","# Set the checkpoint path and predictions file path\n","checkpoint_path=f'/content/drive/MyDrive/Capstone/checkpoints/{checkpoint_name}'\n","predictions_file_path = f'{checkpoint_path}/(test)_{checkpoint_name}_predictions.xlsx' # ÏòàÏ∏° ÌõÑ Î©îÌÉÄ Îç∞Ïù¥ÌÑ∞\n","\n","\n","# Define the model structure\n","model = Model(num_binary_classes=2, num_method_classes=7, model_name=selected_model).to(device)\n","model.load_state_dict(torch.load(f'{checkpoint_path}/{checkpoint_name}.pt', map_location=device))\n","model.eval()\n","model.lstm.train()\n","# print(\"LSTM mode:\", model.lstm.training)  # ‚úÖ Ïó¨Í∏∞Í∞Ä TrueÎ©¥ Îê®\n","\n","\n","folder_list=[ \"Deepfakes\", \"original\",\"Face2Face\", \"FaceShifter\", \"FaceSwap\", \"NeuralTextures\"]\n","\n","for folder_name in folder_list:\n","  input_video_path=f'/content/drive/MyDrive/Capstone/Dataset/ff++(grad-cam_v2)/before/{folder_name}'    # input\n","  output_video_path=f'/content/drive/MyDrive/Capstone/Dataset/ff++(grad-cam_v2)/after/{folder_name}/video'   # ÏòÅÏÉÅ output Ï†ÄÏû•ÌïòÎäî Í≤ΩÎ°ú\n","  frame_path=f'/content/drive/MyDrive/Capstone/Dataset/ff++(grad-cam_v2)/after/{folder_name}/frame'      # jpg output Ï†ÄÏû•ÌïòÎäî Í≤ΩÎ°ú\n","\n","  # Call the function to process videos, compute Grad-CAM, and save frames\n","  process_video_and_save_frames(\n","      input_video_path,\n","      output_video_path,\n","      model=model,\n","      frame_dir=frame_path\n","  )\n"]},{"cell_type":"markdown","source":["##ROI Activation"],"metadata":{"id":"eOfMq6ZnqbFy"},"id":"eOfMq6ZnqbFy"},{"cell_type":"code","source":["def get_bbox(pts):\n","    x, y = pts[:,0], pts[:,1]\n","    return int(x.min()), int(y.min()), int(x.max()), int(y.max())\n","\n","def roi_activation(cam, bbox):\n","    x1, y1, x2, y2 = bbox\n","    patch = cam[y1:y2, x1:x2]\n","    mean_val = float(patch.mean())\n","\n","    if np.isnan(mean_val):\n","        return -1\n","    return mean_val\n","\n","def analyze_roi_activation(input_dir, output_dir_box, base_path, folder_name, model):\n","\n","  fa = face_alignment.FaceAlignment(\n","      face_alignment.LandmarksType.TWO_D,\n","      device=str(device)\n","  )\n","\n","  os.makedirs(output_dir_box, exist_ok=True)\n","\n","  transform = T.Compose([\n","      T.ToTensor(),\n","      T.Resize((224, 224)),\n","      T.Normalize(mean=[0.485, 0.456, 0.406],\n","                  std=[0.229, 0.224, 0.225])\n","  ])\n","\n","  result = []\n","\n","  video_paths = glob.glob(os.path.join(input_dir, '*.mp4'))\n","\n","  for video_path in tqdm(video_paths):\n","\n","    facial_region=['jawline', 'left_eye', 'right_eye', 'left_eye_brow', 'right_eye_brow', 'nose', 'mouth','None']\n","    first_detection_count = {key: 0 for key in facial_region}\n","    second_detection_count = {key: 0 for key in facial_region}\n","    detection_probabillity={key: 0.0 for key in facial_region}\n","\n","\n","\n","    cap = cv2.VideoCapture(video_path)\n","    frame_idx = 0\n","    video_name = os.path.splitext(os.path.basename(video_path))[0]\n","\n","    while cap.isOpened():\n","      success, frame = cap.read()\n","      if not success:\n","        break\n","\n","      rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      landmarks = fa.get_landmarks(rgb)\n","      if not landmarks:\n","        frame_idx += 1\n","        continue\n","      lm = landmarks[0]\n","\n","      # ROI BBoxes\n","      bbox_map = {\n","          'jawline': get_bbox(lm[0:17]),\n","          'left_eye': get_bbox(lm[36:42]),\n","          'right_eye': get_bbox(lm[42:48]),\n","          'left_eye_brow': get_bbox(lm[17:22]),\n","          'right_eye_brow': get_bbox(lm[22:27]),\n","          'nose': get_bbox(lm[27:36]),\n","          'mouth': get_bbox(lm[48:68]),\n","      }\n","\n","      # Grad-CAM\n","      img = transform(rgb).to(device)\n","      cam, cam_score = compute_gradcam_binary(model, img)\n","      cam = cv2.resize(cam, (frame.shape[1], frame.shape[0]))\n","\n","      scores = {region: roi_activation(cam, box) for region, box in bbox_map.items()}\n","      sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n","      first_activated_region = sorted_scores[0][0]\n","      second_activated_region = sorted_scores[1][0]\n","\n","      f_x1, f_y1, f_x2, f_y2 = bbox_map[first_activated_region]\n","      s_x1, s_y1, s_x2, s_y2 = bbox_map[second_activated_region]\n","\n","      # 1Ô∏è‚É£ Grad-CAM ÌûàÌä∏Îßµ Ïò§Î≤ÑÎ†àÏù¥\n","      heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n","      overlay = cv2.addWeighted(frame, 0.6, heatmap, 0.4, 0)\n","\n","      # 2Ô∏è‚É£ ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Î≥µÏÇ¨Ìï¥ÏÑú Î∞ïÏä§Ïö© Ï§ÄÎπÑ\n","      frame_with_box = frame.copy()\n","      overlay_with_box = overlay.copy()\n","\n","      # 3Ô∏è‚É£ Î∞ïÏä§ Í∑∏Î¶¨Í∏∞\n","      if scores[first_activated_region] > 0:\n","        cv2.rectangle(frame_with_box, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)\n","        cv2.rectangle(overlay_with_box, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)\n","      else:\n","        first_activated_region=\"None\"\n","\n","      if scores[second_activated_region] > 0:\n","        cv2.rectangle(frame_with_box, (s_x1, s_y1), (s_x2, s_y2), (255, 0, 0), 2)\n","        cv2.rectangle(overlay_with_box, (s_x1, s_y1), (s_x2, s_y2), (255, 0, 0), 2)\n","      else:\n","        second_activated_region=\"None\"\n","\n","      # 4Ô∏è‚É£ ÌååÏùº Ï†ÄÏû•\n","      file_id = f\"{video_name}_frame{frame_idx:04d}\"\n","      cv2.imwrite(os.path.join(output_dir_box, f\"{file_id}_roi.jpg\"), frame_with_box)\n","      cv2.imwrite(os.path.join(output_dir_box, f\"{file_id}_gradcam.jpg\"), overlay_with_box)\n","\n","      first_detection_count[first_activated_region]+=1\n","      second_detection_count[second_activated_region]+=1\n","      for key in facial_region:\n","        if key!=\"None\" and scores[key]!=-1:\n","          detection_probabillity[key]+=cam_score * scores[key]\n","\n","      result.append({\n","          'file_name': file_id,\n","          'cam_score': cam_score,\n","          'first_activate_region': first_activated_region,\n","          'second_activate_region': second_activated_region,\n","          'f_x1': f_x1, 'f_y1': f_y1, 'f_x2': f_x2, 'f_y2': f_y2,\n","          's_x1': s_x1, 's_y1': s_y1, 's_x2': s_x2, 's_y2': s_y2,\n","          **scores,\n","      })\n","\n","      frame_idx += 1\n","\n","    # Printing all the dictionaries\n","    first_detection_rate = {key: round((value / frame_idx)*100, 2) for key, value in first_detection_count.items()}\n","    second_detection_rate = {key: round((value / frame_idx)*100, 2) for key, value in second_detection_count.items()}\n","\n","    # üìå Note: A high proportion of 'None' may inflate the relative Contribution (%) and should be interpreted with caution.\n","    raw_detection_probabillity= {key: round(value, 4) for key, value in detection_probabillity.items()}\n","    probabillity_total = sum(detection_probabillity.values())\n","    detection_probabillity= {key: round((value/probabillity_total)*100, 2) for key, value in detection_probabillity.items()}\n","\n","\n","\n","    print(\"Video name:\", video_name)\n","    print(\"Facial Region:\", facial_region)\n","    print(\"First Detection Count:\", first_detection_count)\n","    print(\"Second Detection Count:\", second_detection_count)\n","    print(\"First Detection Rate:\", first_detection_rate)\n","    print(\"Second Detection Rate:\", second_detection_rate)\n","    print(\"Raw_Detection Probability:\", raw_detection_probabillity)\n","    print(\"Detection Probability:\", detection_probabillity)\n","\n","  # Ï†ÄÏû•\n","  df = pd.DataFrame(result)\n","  df = df[[\n","      'file_name', 'cam_score',\n","      'first_activate_region', 'second_activate_region',\n","      'f_x1', 'f_y1', 'f_x2', 'f_y2',\n","      's_x1', 's_y1', 's_x2', 's_y2',\n","      'jawline', 'left_eye', 'right_eye',\n","      'left_eye_brow', 'right_eye_brow',\n","      'nose', 'mouth',\n","  ]]\n","\n","  df.to_excel(f\"{base_path}/{folder_name}_roi.xlsx\", index=False)\n","  df.to_json(f\"{base_path}/{folder_name}_roi.json\", orient='records', force_ascii=False)\n"],"metadata":{"id":"pYQ2xxTHM7Hv","executionInfo":{"status":"ok","timestamp":1753472198670,"user_tz":-540,"elapsed":9,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"id":"pYQ2xxTHM7Hv","execution_count":63,"outputs":[]},{"cell_type":"code","execution_count":2,"id":"2f2adcbd","metadata":{"id":"2f2adcbd","executionInfo":{"status":"ok","timestamp":1753473213806,"user_tz":-540,"elapsed":15,"user":{"displayName":"Î∞ïÏßÄÏòÅ","userId":"08079159347381102346"}}},"outputs":[],"source":["\n","checkpoint_name=\"checkpoint_v35\"\n","selected_model=\"EfficientNet-b0\"\n","checkpoint_path=f'/content/drive/MyDrive/Capstone/checkpoints/{checkpoint_name}'\n","\n","# Define the model structure\n","model = Model(num_binary_classes=2, num_method_classes=7, model_name=selected_model).to(device)\n","model.load_state_dict(torch.load(f'{checkpoint_path}/{checkpoint_name}.pt', map_location=device))\n","model.eval()\n","model.lstm.train()\n","# print(\"LSTM mode:\", model.lstm.training)  # ‚úÖ Ïó¨Í∏∞Í∞Ä TrueÎ©¥ Îê®\n","\n","folder_list=[ \"Deepfakes\", \"original\",\"Face2Face\", \"FaceShifter\", \"FaceSwap\", \"NeuralTextures\"]\n","\n","for folder_name in folder_list:\n","  input_dir=f'/content/drive/MyDrive/Capstone/Dataset/ff++(grad-cam_v2)/before/{folder_name}'    # input\n","  output_dir_box=f'/content/drive/MyDrive/Capstone/Dataset/ff++(grad-cam_v2)/after/{folder_name}/roi_frame'      # jpg output Ï†ÄÏû•ÌïòÎäî Í≤ΩÎ°ú\n","  base_path='/content/drive/MyDrive/Capstone/Dataset/ff++(grad-cam_v2)/after'\n","\n","  analyze_roi_activation(\n","      input_dir,\n","      output_dir_box,\n","      base_path,\n","      folder_name,\n","      model=model\n","  )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}