{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_M_hIl_-C6P8"
      },
      "outputs": [],
      "source": [
        "# #before running this please change the RUNTIME to GPU (Runtime -> Change runtime type -> set harware accelarotor as GPU)\n",
        "# #Mount our google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mneMgm9j7ack"
      },
      "source": [
        "Note : Use the drive link for the processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tnFgwZBYCyYR"
      },
      "outputs": [],
      "source": [
        "# #Note : only needed when you have to download the processed data to the environment\n",
        "# #download and unzip the data from google drive Colab environment\n",
        "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "# #use only file id of the link\n",
        "# #Note: Below link is just an example, Not an actual link. Actual Links are in ReadMe file\n",
        "# #https://drive.google.com/file/d/1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07/view?usp=sharing\n",
        "# url = '1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07'\n",
        "# gdd.download_file_from_google_drive(file_id = url,dest_path='./data.zip',unzip=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7hlPaQS4e5VI"
      },
      "outputs": [],
      "source": [
        "#!pip3 install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ### 전체 통합 meta_data랑 global_metadata 생성\n",
        "\n",
        "# import glob\n",
        "# import cv2\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "# # 경로 지정\n",
        "# video_files =  glob.glob('/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/*/*/*/*.mp4')   \n",
        "\n",
        "# # 기준 경로\n",
        "# base_path = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset'\n",
        "\n",
        "# # 데이터 수집\n",
        "# file_name_list = []\n",
        "# folder_path_list = []\n",
        "# label_list = []\n",
        "# split_list = []\n",
        "# dataset_list = []\n",
        "# frame_count_list = []\n",
        "\n",
        "# for video_file in video_files:\n",
        "#     cap = cv2.VideoCapture(video_file)\n",
        "    \n",
        "#     # 파일 이름\n",
        "#     file_name = os.path.basename(video_file)\n",
        "#     file_name_list.append(file_name)\n",
        "\n",
        "#     # 상대 경로\n",
        "#     relative_path = os.path.relpath(video_file, base_path).replace(\"\\\\\", \"/\")\n",
        "#     folder_path_list.append(relative_path)\n",
        "\n",
        "#     # label (real/fake)\n",
        "#     if 'real' in relative_path.lower():\n",
        "#         label = 'REAL'\n",
        "#     elif 'fake' in relative_path.lower():\n",
        "#         label = 'FAKE'\n",
        "#     else:\n",
        "#         label = 'unknown'\n",
        "#     label_list.append(label)\n",
        "\n",
        "#     # split (train/val)\n",
        "#     if '/train/' in relative_path.lower():\n",
        "#         split = 'train'\n",
        "#     elif '/val/' in relative_path.lower():\n",
        "#         split = 'val'\n",
        "#     else:\n",
        "#         split = 'unknown'\n",
        "#     split_list.append(split)\n",
        "\n",
        "#     # dataset (celeb/dfdc)\n",
        "#     if 'fakeavceleb' in relative_path.lower():\n",
        "#         dataset = 'fakeavceleb'\n",
        "#     elif 'dfdc' in relative_path.lower():\n",
        "#         dataset = 'dfdc'\n",
        "#     elif 'celeb'in relative_path.lower():\n",
        "#         dataset = 'celeb'\n",
        "#     elif 'ff++' in relative_path.lower():\n",
        "#         dataset = 'ff++'\n",
        "#     else:\n",
        "#         dataset = 'unknown'\n",
        "#     dataset_list.append(dataset)\n",
        "\n",
        "#     # frame 수\n",
        "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "#     frame_count_list.append(frame_count)\n",
        "\n",
        "#     cap.release()\n",
        "\n",
        "# # 데이터프레임 생성\n",
        "# df = pd.DataFrame({\n",
        "#     'file_name': file_name_list,\n",
        "#     'folder_path': folder_path_list,\n",
        "#     'label': label_list,\n",
        "#     'split': split_list,\n",
        "#     'dataset': dataset_list,\n",
        "#     'frame': frame_count_list\n",
        "# })\n",
        "\n",
        "# df2 = pd.DataFrame({\n",
        "#     'file_name': file_name_list,\n",
        "#     'label': label_list,\n",
        "# })\n",
        "\n",
        "# # 저장할 엑셀 파일 경로\n",
        "# output_excel_path = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/global_meta_data.xlsx'\n",
        "# output_csv_path = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Global_metadata.csv'\n",
        "\n",
        "# # 엑셀로 저장\n",
        "# df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
        "# print(f\"✅ 메타데이터 생성 완료: {output_excel_path}\")\n",
        "\n",
        "# # CSV로 저장\n",
        "# df2.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# print(f\"✅ 파일 정보가 CSV로 저장되었습니다: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_file_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/train/*'\n",
        "input_file_path2=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/DFDC/train/*'\n",
        "input_file_path3=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/FakeAVCeleb_v1.2/train/*'\n",
        "input_file_path4=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/deepspeak/train/*'\n",
        "input_file_path5=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/celeb/train/*'\n",
        "\n",
        "meta_data_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset'\n",
        "checkpoint_path=f'/Users/jiyeong/HUFS.CSE.DE-fake-it/model/checkpoints'\n",
        "frames=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import libraries\n",
        "#!pip3 install face_recognition\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import sys\n",
        "from torch import nn\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QZ22Sj8d0JoT"
      },
      "outputs": [],
      "source": [
        "#THis code is to check if the video is corrupted or not..\n",
        "#If the video is corrupted delete the video.\n",
        "import glob\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "#Check if the file is corrupted or not\n",
        "def validate_video(vid_path,train_transforms):\n",
        "      transform = train_transforms\n",
        "      count = 20\n",
        "      video_path = vid_path\n",
        "      frames = []\n",
        "      a = int(100/count)\n",
        "      first_frame = np.random.randint(0,a)\n",
        "      temp_video = video_path.split('/')[-1]\n",
        "      for i,frame in enumerate(frame_extract(video_path)):\n",
        "        frames.append(transform(frame))\n",
        "        if(len(frames) == count):\n",
        "          break\n",
        "      frames = torch.stack(frames)\n",
        "      frames = frames[:count]\n",
        "      return frames\n",
        "#extract a from from video\n",
        "def frame_extract(path):\n",
        "  vidObj = cv2.VideoCapture(path) \n",
        "  success = 1\n",
        "  while success:\n",
        "      success, image = vidObj.read()\n",
        "      if success:\n",
        "          yield image\n",
        "\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "\n",
        "video_fil = glob.glob(f'{input_file_path}/*.mp4')  \n",
        "# video_fil += glob.glob(f'{input_file_path2}/*.mp4') # 경로 변경\n",
        "# video_fil += glob.glob(f'{input_file_path3}/*.mp4')\n",
        "# video_fil += glob.glob(f'{input_file_path4}/*.mp4')\n",
        "# video_fil += glob.glob(f'{input_file_path5}/*.mp4')\n",
        "# video_fil += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n",
        "# video_fil += glob.glob('/content/drive/My Drive/FF_Face_only_data/*.mp4')\n",
        "print(\"Total no of videos :\" , len(video_fil))\n",
        "print(video_fil)\n",
        "count = 0\n",
        "for i in video_fil:\n",
        "  try:\n",
        "    count+=1\n",
        "    validate_video(i,train_transforms)\n",
        "  except:\n",
        "    print(\"Number of video processed: \" , count ,\" Remaining : \" , (len(video_fil) - count))\n",
        "    print(\"Corrupted video is : \" , i)\n",
        "    continue\n",
        "print((len(video_fil) - count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CEIygy8uDFXc"
      },
      "outputs": [],
      "source": [
        "#to load preprocessod video to memory\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import random\n",
        "\n",
        "\n",
        "video_files = glob.glob(f'{input_file_path}/*.mp4')\n",
        "# video_files = glob.glob(f'{input_file_path2}/*.mp4')    # 경로 변경\n",
        "# video_files += glob.glob(f'{input_file_path3}/*.mp4')\n",
        "# video_files += glob.glob(f'{input_file_path4}/*.mp4')\n",
        "# video_files += glob.glob(f'{input_file_path5}/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_FAKE_Face_only_data/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/FF_Face_only_data/*.mp4')\n",
        "random.shuffle(video_files)\n",
        "random.shuffle(video_files)\n",
        "\n",
        "frame_count = []\n",
        "short_frame=[]\n",
        "print(len(video_files))\n",
        "\n",
        "for video_file in reversed(video_files): # 이거 앞에서 부터 하면 remove로 인해 frame_count랑 video_files 길이가 달라짐, 그래서 reversed 추가하여 뒤에서 부터 탐색!!\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<frames):  # frames 변수 위에서 조정\n",
        "    video_files.remove(video_file)\n",
        "    short_frame.append(video_file)\n",
        "    continue\n",
        "\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "  \n",
        "print(\"frames are \" , frame_count)\n",
        "print(\"Total no of video: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))\n",
        "print('Short_frame_count : ', len(short_frame))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OqGXNkqhDKZU"
      },
      "outputs": [],
      "source": [
        "# load the video name and labels from csv\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "class video_dataset(Dataset):\n",
        "    def __init__(self,video_names,labels,sequence_length = 60,transform = None):\n",
        "        self.video_names = video_names\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.count = sequence_length\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "    def __getitem__(self,idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        frames = []\n",
        "        a = int(100/self.count)\n",
        "        first_frame = np.random.randint(0,a)\n",
        "        temp_video = video_path.split('/')[-1]\n",
        "        #print(temp_video)\n",
        "        label = self.labels.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
        "        if(label == 'FAKE'):\n",
        "          label = 0\n",
        "        if(label == 'REAL'):\n",
        "          label = 1\n",
        "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
        "          frames.append(self.transform(frame))\n",
        "          if(len(frames) == self.count):\n",
        "            break\n",
        "        frames = torch.stack(frames)\n",
        "        frames = frames[:self.count]\n",
        "        #print(\"length:\" , len(frames), \"label\",label)\n",
        "        return frames,label\n",
        "    def frame_extract(self,path):\n",
        "      vidObj = cv2.VideoCapture(path) \n",
        "      success = 1\n",
        "      while success:\n",
        "          success, image = vidObj.read()\n",
        "          if success:\n",
        "              yield image\n",
        "#plot the image\n",
        "def im_plot(tensor):\n",
        "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
        "    b,g,r = cv2.split(image)\n",
        "    image = cv2.merge((r,g,b))\n",
        "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
        "    image = image*255.0\n",
        "    plt.imshow(image.astype(int))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1leMozhXa5LF"
      },
      "outputs": [],
      "source": [
        "#count the number of fake and real videos\n",
        "def number_of_real_and_fake_videos(data_list):\n",
        "  header_list = [\"file\",\"label\"]\n",
        "  lab = pd.read_csv(f'{meta_data_path}/Global_metadata.csv',names=header_list)\n",
        "  fake = 0\n",
        "  real = 0\n",
        "  for i in data_list:\n",
        "    temp_video = i.split('/')[-1]\n",
        "    label = lab.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
        "    if(label == 'FAKE'):\n",
        "      fake+=1\n",
        "    if(label == 'REAL'):\n",
        "      real+=1\n",
        "  return real,fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sWMZn0YHDO2b"
      },
      "outputs": [],
      "source": [
        "# load the labels and video in data loader\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "header_list = [\"file\",\"label\"]\n",
        "labels = pd.read_csv(f'{meta_data_path}/Global_metadata.csv',names=header_list)\n",
        "#print(labels)\n",
        "\n",
        "train_videos = video_files[:int(0.7*len(video_files))]  # 7:3으로 train:test\n",
        "valid_videos = video_files[int(0.7*len(video_files)):]\n",
        "print(\"train : \" , len(train_videos))\n",
        "print(\"test : \" , len(valid_videos))\n",
        "# train_videos,valid_videos = train_test_split(data,test_size = 0.2)\n",
        "# print(train_videos)\n",
        "\n",
        "print(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(train_videos)[0],\" Fake:\",number_of_real_and_fake_videos(train_videos)[1])\n",
        "print(\"TEST: \", \"Real:\",number_of_real_and_fake_videos(valid_videos)[0],\" Fake:\",number_of_real_and_fake_videos(valid_videos)[1])\n",
        "\n",
        "\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "train_data = video_dataset(train_videos,labels,sequence_length = 10,transform = train_transforms)\n",
        "#print(train_data)\n",
        "val_data = video_dataset(valid_videos,labels,sequence_length = 10,transform = train_transforms)\n",
        "# cpu사용하기 때문에 병렬처리 뻄\n",
        "# train_loader = DataLoader(train_data,batch_size = 4,shuffle = True,num_workers = 4)\n",
        "# valid_loader = DataLoader(val_data,batch_size = 4,shuffle = True,num_workers = 4)\n",
        "train_loader = DataLoader(train_data,batch_size = 32,shuffle = True,num_workers = 0)  # 여기서 batch size 조정 (한번에 몇개의 데이터를 묶어서 학습할지, batch개수=데이터 수/batch size)\n",
        "valid_loader = DataLoader(val_data,batch_size = 32,shuffle = True,num_workers = 0)\n",
        "image,label = train_data[0]\n",
        "im_plot(image[0,:,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UtOXSqyBDRnD"
      },
      "outputs": [],
      "source": [
        "#Model with feature visualization\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True) #Residual Network CNN\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048,num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        batch_size,seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size,seq_length,2048)\n",
        "        x_lstm,_ = self.lstm(x,None)\n",
        "        return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WYNhn10tDV90"
      },
      "outputs": [],
      "source": [
        "# model = Model(2).cuda()\n",
        "# a,b = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.cuda.FloatTensor))\n",
        "\n",
        "# cuda 사용 안되서 cpu작동으로 바꿈\n",
        "# model = Model(2).cpu()\n",
        "# a, b = model(torch.from_numpy(np.empty((1, 20, 3, 112, 112))).type(torch.FloatTensor))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# 모델을 MPS로 보내기\n",
        "model = Model(2).to(device)\n",
        "\n",
        "# 입력 텐서도 MPS로 보내기\n",
        "input_tensor = torch.from_numpy(np.empty((1, 20, 3, 112, 112))).type(torch.FloatTensor).to(device)\n",
        "\n",
        "# 모델 실행\n",
        "a, b = model(input_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FKheLUWBDaNN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    t = []\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        # GPU에서 실행 안함\n",
        "        # if torch.cuda.is_available():\n",
        "        #     targets = targets.type(torch.cuda.LongTensor)\n",
        "        #     inputs = inputs.cuda()\n",
        "\n",
        "        # inputs, targets MPS로 올리기\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        _,outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc = calculate_accuracy(outputs, targets)\n",
        "        # loss  = criterion(outputs,targets.type(torch.cuda.LongTensor))\n",
        "        # acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    num_epochs,\n",
        "                    i,\n",
        "                    len(data_loader),\n",
        "                    losses.avg,\n",
        "                    accuracies.avg))\n",
        "\n",
        "    # 모델 저장\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)  # 폴더 없으면 생성\n",
        "    # torch.save(model.state_dict(),'/content/checkpoint.pt')\n",
        "    torch.save(model.state_dict(), f'{checkpoint_path}/checkpoint.pt')\n",
        "\n",
        "    return losses.avg,accuracies.avg\n",
        "\n",
        "def test(epoch,model, data_loader ,criterion):\n",
        "    print('Testing')\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    pred = []\n",
        "    true = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            # if torch.cuda.is_available():\n",
        "                # targets = targets.cuda().type(torch.cuda.FloatTensor)\n",
        "                # inputs = inputs.cuda()\n",
        "\n",
        "            #  GPu 안되서 바꿈\n",
        "            # inputs = inputs.cpu()  # inputs를 CPU로\n",
        "            # targets = targets.cpu()  # targets를 CPU로\n",
        "\n",
        "            # inputs, targets MPS로 올리기 (CPU보다 빠름)\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            #\n",
        "\n",
        "            _,outputs = model(inputs)\n",
        "            # GPu 안되서 바꿈\n",
        "            # loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n",
        "            # acc = calculate_accuracy(outputs,targets.type(torch.cuda.LongTensor))\n",
        "            loss = torch.mean(criterion(outputs, targets))\n",
        "            acc = calculate_accuracy(outputs, targets)\n",
        "            #\n",
        "            _,p = torch.max(outputs,1) \n",
        "            # true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
        "            true += (targets.type(torch.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
        "\n",
        "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            accuracies.update(acc, inputs.size(0))\n",
        "            sys.stdout.write(\n",
        "                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
        "                    % (\n",
        "                        i,\n",
        "                        len(data_loader),\n",
        "                        losses.avg,\n",
        "                        accuracies.avg\n",
        "                        )\n",
        "                    )\n",
        "        print('\\nAccuracy {}'.format(accuracies.avg))\n",
        "    return true,pred,losses.avg,accuracies.avg\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    batch_size = targets.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(1, 1, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1, -1))\n",
        "    n_correct_elems = correct.float().sum().item()\n",
        "    return 100* n_correct_elems / batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "b8WneBZNfysN"
      },
      "outputs": [],
      "source": [
        "#Output confusion matrix   성능 평가\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix  #내가 추가함\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "def print_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    print('True positive = ', cm[0][0])\n",
        "    print('False positive = ', cm[0][1])\n",
        "    print('False negative = ', cm[1][0])\n",
        "    print('True negative = ', cm[1][1])\n",
        "    print('\\n')\n",
        "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "    sn.set(font_scale=1.4) # for label size\n",
        "    sn.heatmap(df_cm, annot=True,fmt='d', annot_kws={\"size\": 16}) # font size ,fmt='d'로 정수 표현\n",
        "    plt.ylabel('Actual label', size = 20)\n",
        "    plt.xlabel('Predicted label', size = 20)\n",
        "    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.ylim([2, 0])\n",
        "    plt.show()\n",
        "    calculated_acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+ cm[1][1])\n",
        "    print(\"Calculated Accuracy\",calculated_acc*100)\n",
        "\n",
        "\n",
        "    y_true = (['Fake'] * sum(cm[0]) + ['Real'] * sum(cm[1]))\n",
        "    y_pred = (['Fake'] * cm[0][0] + ['Real'] * cm[0][1] +\n",
        "            ['Fake'] * cm[1][0] + ['Real'] * cm[1][1])\n",
        "\n",
        "    # 성능 출력\n",
        "    print(\"📊 Confusion Matrix:\\n\", cm)\n",
        "    print(\"\\n📈 Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Fake', 'Real']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fExJLjt2AtV9"
      },
      "outputs": [],
      "source": [
        "# loss 그래프\n",
        "def plot_loss(train_loss_avg,test_loss_avg,num_epochs):\n",
        "  loss_train = train_loss_avg\n",
        "  loss_val = test_loss_avg\n",
        "  print(num_epochs)\n",
        "  epochs = range(1,num_epochs+1)\n",
        "  plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "  plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "def plot_accuracy(train_accuracy,test_accuracy,num_epochs):\n",
        "  loss_train = train_accuracy\n",
        "  loss_val = test_accuracy\n",
        "  epochs = range(1,num_epochs+1)\n",
        "  plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "  plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "  plt.title('Training and Validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rUe1XrYnDdit"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "#learning rate\n",
        "lr = 1e-4              #시작 1e-5#0.001\n",
        "#number of epochs \n",
        "num_epochs = 5\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr,weight_decay = 1e-5)\n",
        "\n",
        "#class_weights = torch.from_numpy(np.asarray([1,15])).type(torch.FloatTensor).cuda()\n",
        "#criterion = nn.CrossEntropyLoss(weight = class_weights).cuda()\n",
        "\n",
        "#GPU안되서 수정\n",
        "# criterion = nn.CrossEntropyLoss().cuda()\n",
        "#criterion = nn.CrossEntropyLoss().cpu()  # CPU 사용 버전\n",
        "criterion = nn.CrossEntropyLoss().to(device) #MPS 사용버전\n",
        "#\n",
        "train_loss_avg =[]\n",
        "train_accuracy = []\n",
        "test_loss_avg = []\n",
        "test_accuracy = []\n",
        "\n",
        "# 시간 측정 시작\n",
        "start_time = time.time()\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    l, acc = train_epoch(epoch,num_epochs,train_loader,model,criterion,optimizer)\n",
        "    train_loss_avg.append(l)\n",
        "    train_accuracy.append(acc)\n",
        "    true,pred,tl,t_acc = test(epoch,model,valid_loader,criterion)\n",
        "    test_loss_avg.append(tl)\n",
        "    test_accuracy.append(t_acc)\n",
        "    \n",
        "    epoch_end_time = time.time()\n",
        "    epoch_elapsed = epoch_end_time - epoch_start_time\n",
        "    print(f\"✅ Epoch {epoch} 소요 시간: {epoch_elapsed:.2f}초\")\n",
        "# 시간 측정 끝\n",
        "end_time = time.time()\n",
        "    \n",
        "plot_loss(train_loss_avg,test_loss_avg,len(train_loss_avg))\n",
        "plot_accuracy(train_accuracy,test_accuracy,len(train_accuracy))\n",
        "print(confusion_matrix(true,pred))\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"✅ 전체 학습 소요 시간: {elapsed_time:.2f}초\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_confusion_matrix(true,pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습률 1e-5 -> 1e-4 \n",
        "# epoch 10 -> 20\n",
        "# 정확도 60->80%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# raise Exception(\"중간 점검: 이후 셀 실행을 막습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_input_file_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/train/*'\n",
        "# test_output_file_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/val/*'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Model with feature visualization\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True) #Residual Network CNN\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048,num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        batch_size,seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size,seq_length,2048)\n",
        "        x_lstm,_ = self.lstm(x,None)\n",
        "        return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 디바이스 설정\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# 1. 모델 구조를 다시 정의\n",
        "model = Model(num_classes=2).to(device)\n",
        "\n",
        "# 2. checkpoint 불러오기\n",
        "model.load_state_dict(torch.load(f'{checkpoint_path}/checkpoint.pt'))\n",
        "\n",
        "# 3. 평가 모드 전환\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "new_video_files =  glob.glob(f'{test_input_file_path}/*.mp4')   # 경로 변경\n",
        "# new_video_files += glob.glob(f'{test_output_file_path}/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_FAKE_Face_only_data/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n",
        "random.shuffle(new_video_files)\n",
        "random.shuffle(new_video_files)\n",
        "\n",
        "frame_count = []\n",
        "short_frame=[]\n",
        "\n",
        "for video_file in reversed(new_video_files): # 이거 앞에서 부터 하면 remove로 인해 frame_count랑 video_files 길이가 달라짐, 그래서 reversed 추가하여 뒤에서 부터 탐색!!\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<frames):  # frames 변수 위에서 조정\n",
        "    new_video_files.remove(video_file)\n",
        "    short_frame.append(video_file)\n",
        "    continue\n",
        "\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "  \n",
        "print(\"frames are \" , frame_count)\n",
        "print(\"Total no of video: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))\n",
        "print('Short_frame_count : ', len(short_frame))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# 결과 저장 리스트\n",
        "results = []\n",
        "label_list = []\n",
        "folder_path_list=[]\n",
        "base_path = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset'\n",
        "\n",
        "with torch.no_grad():\n",
        "    for video_path in tqdm(new_video_files):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_preds = []\n",
        "        frame_idx = 0\n",
        "\n",
        "        relative_path = os.path.relpath(video_path,base_path).replace(\"\\\\\", \"/\")\n",
        "        folder_path_list.append(relative_path)\n",
        "\n",
        "        # label (real/fake)\n",
        "        if 'real' in relative_path.lower():\n",
        "            label = 'REAL'\n",
        "        elif 'fake' in relative_path.lower():\n",
        "            label = 'FAKE'\n",
        "        else:\n",
        "            label = 'unknown'\n",
        "        label_list.append(label)\n",
        "        success, frame = cap.read()\n",
        "        while success:\n",
        "            frame_idx += 1\n",
        "            if frame_idx % 5 == 0:  # 매 5번째 프레임만 뽑아서 예측 (속도 + 대표성)\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                input_tensor = transform(frame)\n",
        "                input_tensor = input_tensor.unsqueeze(0).unsqueeze(0)  # (batch=1, seq_len=1, c=3, h, w)\n",
        "                input_tensor = input_tensor.to(device).float()\n",
        "\n",
        "                fmap, outputs = model(input_tensor)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                frame_preds.append(predicted.item())\n",
        "\n",
        "            success, frame = cap.read()\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # 비디오 하나에 대한 최종 예측\n",
        "        if len(frame_preds) == 0:\n",
        "            final_prediction = 'Unknown'\n",
        "        else:\n",
        "            majority = round(sum(frame_preds) / len(frame_preds))  # 다수결\n",
        "            final_prediction = 'REAL' if majority == 1 else 'FAKE'\n",
        "\n",
        "        results.append({\n",
        "            'Filename': os.path.basename(video_path),\n",
        "            'Filepath': video_path,\n",
        "            'label': label,\n",
        "            'Prediction': final_prediction\n",
        "        })\n",
        "\n",
        "# 결과 DataFrame으로 만들기\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# 엑셀로 저장\n",
        "output_excel_path = f'{meta_data_path}/ff+(train)_video_predictions.xlsx'\n",
        "df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"✅ 모든 비디오 예측 결과가 엑셀로 저장되었습니다: {output_excel_path}\")\n",
        "\n",
        "# Confusion Matrix 계산\n",
        "labels = ['REAL', 'FAKE']\n",
        "y_true = label_list  # 실제 레이블\n",
        "y_pred = [result['Prediction'] for result in results]  # 예측 레이블\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_confusion_matrix(y_true,y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model_and_train_csv.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ba2023",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
