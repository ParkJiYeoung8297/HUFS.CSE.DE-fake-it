{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_M_hIl_-C6P8"
      },
      "outputs": [],
      "source": [
        "# #before running this please change the RUNTIME to GPU (Runtime -> Change runtime type -> set harware accelarotor as GPU)\n",
        "# #Mount our google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mneMgm9j7ack"
      },
      "source": [
        "Note : Use the drive link for the processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tnFgwZBYCyYR"
      },
      "outputs": [],
      "source": [
        "# #Note : only needed when you have to download the processed data to the environment\n",
        "# #download and unzip the data from google drive Colab environment\n",
        "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "# #use only file id of the link\n",
        "# #Note: Below link is just an example, Not an actual link. Actual Links are in ReadMe file\n",
        "# #https://drive.google.com/file/d/1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07/view?usp=sharing\n",
        "# url = '1ubvKLzBDe5i1acxgGUK6ObeNBYCKUS07'\n",
        "# gdd.download_file_from_google_drive(file_id = url,dest_path='./data.zip',unzip=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7hlPaQS4e5VI"
      },
      "outputs": [],
      "source": [
        "#!pip3 install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ### ì „ì²´ í†µí•© meta_dataë‘ global_metadata ìƒì„±\n",
        "\n",
        "# import glob\n",
        "# import cv2\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "# # ê²½ë¡œ ì§€ì •\n",
        "# video_files =  glob.glob('/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/*/*/*/*.mp4')   \n",
        "\n",
        "# # ê¸°ì¤€ ê²½ë¡œ\n",
        "# base_path = '/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset'\n",
        "\n",
        "# # ë°ì´í„° ìˆ˜ì§‘\n",
        "# file_name_list = []\n",
        "# folder_path_list = []\n",
        "# label_list = []\n",
        "# split_list = []\n",
        "# dataset_list = []\n",
        "# frame_count_list = []\n",
        "\n",
        "# for video_file in video_files:\n",
        "#     cap = cv2.VideoCapture(video_file)\n",
        "    \n",
        "#     # íŒŒì¼ ì´ë¦„\n",
        "#     file_name = os.path.basename(video_file)\n",
        "#     file_name_list.append(file_name)\n",
        "\n",
        "#     # ìƒëŒ€ ê²½ë¡œ\n",
        "#     relative_path = os.path.relpath(video_file, base_path).replace(\"\\\\\", \"/\")\n",
        "#     folder_path_list.append(relative_path)\n",
        "\n",
        "#     # label (real/fake)\n",
        "#     if 'real' in relative_path.lower():\n",
        "#         label = 'REAL'\n",
        "#     elif 'fake' in relative_path.lower():\n",
        "#         label = 'FAKE'\n",
        "#     else:\n",
        "#         label = 'unknown'\n",
        "#     label_list.append(label)\n",
        "\n",
        "#     # split (train/val)\n",
        "#     if '/train/' in relative_path.lower():\n",
        "#         split = 'train'\n",
        "#     elif '/val/' in relative_path.lower():\n",
        "#         split = 'val'\n",
        "#     else:\n",
        "#         split = 'unknown'\n",
        "#     split_list.append(split)\n",
        "\n",
        "#     # dataset (celeb/dfdc)\n",
        "#     if 'fakeavceleb' in relative_path.lower():\n",
        "#         dataset = 'fakeavceleb'\n",
        "#     elif 'dfdc' in relative_path.lower():\n",
        "#         dataset = 'dfdc'\n",
        "#     elif 'celeb'in relative_path.lower():\n",
        "#         dataset = 'celeb'\n",
        "#     elif 'ff++' in relative_path.lower():\n",
        "#         dataset = 'ff++'\n",
        "#     else:\n",
        "#         dataset = 'unknown'\n",
        "#     dataset_list.append(dataset)\n",
        "\n",
        "#     # frame ìˆ˜\n",
        "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "#     frame_count_list.append(frame_count)\n",
        "\n",
        "#     cap.release()\n",
        "\n",
        "# # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "# df = pd.DataFrame({\n",
        "#     'file_name': file_name_list,\n",
        "#     'folder_path': folder_path_list,\n",
        "#     'label': label_list,\n",
        "#     'split': split_list,\n",
        "#     'dataset': dataset_list,\n",
        "#     'frame': frame_count_list\n",
        "# })\n",
        "\n",
        "# df2 = pd.DataFrame({\n",
        "#     'file_name': file_name_list,\n",
        "#     'label': label_list,\n",
        "# })\n",
        "\n",
        "# # ì €ì¥í•  ì—‘ì…€ íŒŒì¼ ê²½ë¡œ\n",
        "# output_excel_path = '/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/global_meta_data.xlsx'\n",
        "# output_csv_path = '/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/Global_metadata.csv'\n",
        "\n",
        "# # ì—‘ì…€ë¡œ ì €ì¥\n",
        "# df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
        "# print(f\"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {output_excel_path}\")\n",
        "\n",
        "# # CSVë¡œ ì €ì¥\n",
        "# df2.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# print(f\"âœ… íŒŒì¼ ì •ë³´ê°€ CSVë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_file_path=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/ff++/train/*'\n",
        "input_file_path2=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/DFDC/train/*'\n",
        "input_file_path3=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/FakeAVCeleb_v1.2/train/*'\n",
        "input_file_path4=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/deepspeak/train/*'\n",
        "input_file_path5=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/celeb/train/*'\n",
        "\n",
        "meta_data_path=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset'\n",
        "checkpoint_path=f'/Users/jiyeong/HUFS.CSE.DE-fake-it/model/checkpoints'\n",
        "frames=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import libraries\n",
        "#!pip3 install face_recognition\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import sys\n",
        "from torch import nn\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QZ22Sj8d0JoT"
      },
      "outputs": [],
      "source": [
        "#THis code is to check if the video is corrupted or not..\n",
        "#If the video is corrupted delete the video.\n",
        "import glob\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "#Check if the file is corrupted or not\n",
        "def validate_video(vid_path,train_transforms):\n",
        "      transform = train_transforms\n",
        "      count = 20\n",
        "      video_path = vid_path\n",
        "      frames = []\n",
        "      a = int(100/count)\n",
        "      first_frame = np.random.randint(0,a)\n",
        "      temp_video = video_path.split('/')[-1]\n",
        "      for i,frame in enumerate(frame_extract(video_path)):\n",
        "        frames.append(transform(frame))\n",
        "        if(len(frames) == count):\n",
        "          break\n",
        "      frames = torch.stack(frames)\n",
        "      frames = frames[:count]\n",
        "      return frames\n",
        "#extract a from from video\n",
        "def frame_extract(path):\n",
        "  vidObj = cv2.VideoCapture(path) \n",
        "  success = 1\n",
        "  while success:\n",
        "      success, image = vidObj.read()\n",
        "      if success:\n",
        "          yield image\n",
        "\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "\n",
        "video_fil = glob.glob(f'{input_file_path}/*.mp4')  \n",
        "# video_fil += glob.glob(f'{input_file_path2}/*.mp4') # ê²½ë¡œ ë³€ê²½\n",
        "# video_fil += glob.glob(f'{input_file_path3}/*.mp4')\n",
        "# video_fil += glob.glob(f'{input_file_path4}/*.mp4')\n",
        "# video_fil += glob.glob(f'{input_file_path5}/*.mp4')\n",
        "# video_fil += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n",
        "# video_fil += glob.glob('/content/drive/My Drive/FF_Face_only_data/*.mp4')\n",
        "print(\"Total no of videos :\" , len(video_fil))\n",
        "print(video_fil)\n",
        "count = 0\n",
        "for i in video_fil:\n",
        "  try:\n",
        "    count+=1\n",
        "    validate_video(i,train_transforms)\n",
        "  except:\n",
        "    print(\"Number of video processed: \" , count ,\" Remaining : \" , (len(video_fil) - count))\n",
        "    print(\"Corrupted video is : \" , i)\n",
        "    continue\n",
        "print((len(video_fil) - count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CEIygy8uDFXc"
      },
      "outputs": [],
      "source": [
        "#to load preprocessod video to memory\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import random\n",
        "\n",
        "\n",
        "video_files = glob.glob(f'{input_file_path}/*.mp4')\n",
        "# video_files = glob.glob(f'{input_file_path2}/*.mp4')    # ê²½ë¡œ ë³€ê²½\n",
        "# video_files += glob.glob(f'{input_file_path3}/*.mp4')\n",
        "# video_files += glob.glob(f'{input_file_path4}/*.mp4')\n",
        "# video_files += glob.glob(f'{input_file_path5}/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_FAKE_Face_only_data/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/FF_Face_only_data/*.mp4')\n",
        "random.shuffle(video_files)\n",
        "random.shuffle(video_files)\n",
        "\n",
        "frame_count = []\n",
        "short_frame=[]\n",
        "print(len(video_files))\n",
        "\n",
        "for video_file in reversed(video_files): # ì´ê±° ì•ì—ì„œ ë¶€í„° í•˜ë©´ removeë¡œ ì¸í•´ frame_countë‘ video_files ê¸¸ì´ê°€ ë‹¬ë¼ì§, ê·¸ë˜ì„œ reversed ì¶”ê°€í•˜ì—¬ ë’¤ì—ì„œ ë¶€í„° íƒìƒ‰!!\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<frames):  # frames ë³€ìˆ˜ ìœ„ì—ì„œ ì¡°ì •\n",
        "    video_files.remove(video_file)\n",
        "    short_frame.append(video_file)\n",
        "    continue\n",
        "\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "  \n",
        "print(\"frames are \" , frame_count)\n",
        "print(\"Total no of video: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))\n",
        "print('Short_frame_count : ', len(short_frame))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OqGXNkqhDKZU"
      },
      "outputs": [],
      "source": [
        "# load the video name and labels from csv\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "class video_dataset(Dataset):\n",
        "    def __init__(self,video_names,labels,sequence_length = 60,transform = None):\n",
        "        self.video_names = video_names\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.count = sequence_length\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "    def __getitem__(self,idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        frames = []\n",
        "        a = int(100/self.count)\n",
        "        first_frame = np.random.randint(0,a)\n",
        "        temp_video = video_path.split('/')[-1]\n",
        "        #print(temp_video)\n",
        "        label = self.labels.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
        "        if(label == 'FAKE'):\n",
        "          label = 0\n",
        "        if(label == 'REAL'):\n",
        "          label = 1\n",
        "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
        "          frames.append(self.transform(frame))\n",
        "          if(len(frames) == self.count):\n",
        "            break\n",
        "        frames = torch.stack(frames)\n",
        "        frames = frames[:self.count]\n",
        "        #print(\"length:\" , len(frames), \"label\",label)\n",
        "        return frames,label\n",
        "    def frame_extract(self,path):\n",
        "      vidObj = cv2.VideoCapture(path) \n",
        "      success = 1\n",
        "      while success:\n",
        "          success, image = vidObj.read()\n",
        "          if success:\n",
        "              yield image\n",
        "#plot the image\n",
        "def im_plot(tensor):\n",
        "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
        "    b,g,r = cv2.split(image)\n",
        "    image = cv2.merge((r,g,b))\n",
        "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
        "    image = image*255.0\n",
        "    plt.imshow(image.astype(int))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1leMozhXa5LF"
      },
      "outputs": [],
      "source": [
        "#count the number of fake and real videos\n",
        "def number_of_real_and_fake_videos(data_list):\n",
        "  header_list = [\"file\",\"label\"]\n",
        "  lab = pd.read_csv(f'{meta_data_path}/Global_metadata.csv',names=header_list)\n",
        "  fake = 0\n",
        "  real = 0\n",
        "  for i in data_list:\n",
        "    temp_video = i.split('/')[-1]\n",
        "    label = lab.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
        "    if(label == 'FAKE'):\n",
        "      fake+=1\n",
        "    if(label == 'REAL'):\n",
        "      real+=1\n",
        "  return real,fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sWMZn0YHDO2b"
      },
      "outputs": [],
      "source": [
        "# load the labels and video in data loader\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "header_list = [\"file\",\"label\"]\n",
        "labels = pd.read_csv(f'{meta_data_path}/Global_metadata.csv',names=header_list)\n",
        "#print(labels)\n",
        "\n",
        "train_videos = video_files[:int(0.7*len(video_files))]  # 7:3ìœ¼ë¡œ train:test\n",
        "valid_videos = video_files[int(0.7*len(video_files)):]\n",
        "print(\"train : \" , len(train_videos))\n",
        "print(\"test : \" , len(valid_videos))\n",
        "# train_videos,valid_videos = train_test_split(data,test_size = 0.2)\n",
        "# print(train_videos)\n",
        "\n",
        "print(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(train_videos)[0],\" Fake:\",number_of_real_and_fake_videos(train_videos)[1])\n",
        "print(\"TEST: \", \"Real:\",number_of_real_and_fake_videos(valid_videos)[0],\" Fake:\",number_of_real_and_fake_videos(valid_videos)[1])\n",
        "\n",
        "\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "train_data = video_dataset(train_videos,labels,sequence_length = 10,transform = train_transforms)\n",
        "#print(train_data)\n",
        "val_data = video_dataset(valid_videos,labels,sequence_length = 10,transform = train_transforms)\n",
        "# cpuì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬ì²˜ë¦¬ ë»„\n",
        "# train_loader = DataLoader(train_data,batch_size = 4,shuffle = True,num_workers = 4)\n",
        "# valid_loader = DataLoader(val_data,batch_size = 4,shuffle = True,num_workers = 4)\n",
        "train_loader = DataLoader(train_data,batch_size = 32,shuffle = True,num_workers = 0)  # ì—¬ê¸°ì„œ batch size ì¡°ì • (í•œë²ˆì— ëª‡ê°œì˜ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ í•™ìŠµí• ì§€, batchê°œìˆ˜=ë°ì´í„° ìˆ˜/batch size)\n",
        "valid_loader = DataLoader(val_data,batch_size = 32,shuffle = True,num_workers = 0)\n",
        "image,label = train_data[0]\n",
        "im_plot(image[0,:,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UtOXSqyBDRnD"
      },
      "outputs": [],
      "source": [
        "#Model with feature visualization\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True) #Residual Network CNN\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048,num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        batch_size,seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size,seq_length,2048)\n",
        "        x_lstm,_ = self.lstm(x,None)\n",
        "        return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WYNhn10tDV90"
      },
      "outputs": [],
      "source": [
        "# model = Model(2).cuda()\n",
        "# a,b = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.cuda.FloatTensor))\n",
        "\n",
        "# cuda ì‚¬ìš© ì•ˆë˜ì„œ cpuì‘ë™ìœ¼ë¡œ ë°”ê¿ˆ\n",
        "# model = Model(2).cpu()\n",
        "# a, b = model(torch.from_numpy(np.empty((1, 20, 3, 112, 112))).type(torch.FloatTensor))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"âœ… Using device: {device}\")\n",
        "\n",
        "# ëª¨ë¸ì„ MPSë¡œ ë³´ë‚´ê¸°\n",
        "model = Model(2).to(device)\n",
        "\n",
        "# ì…ë ¥ í…ì„œë„ MPSë¡œ ë³´ë‚´ê¸°\n",
        "input_tensor = torch.from_numpy(np.empty((1, 20, 3, 112, 112))).type(torch.FloatTensor).to(device)\n",
        "\n",
        "# ëª¨ë¸ ì‹¤í–‰\n",
        "a, b = model(input_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FKheLUWBDaNN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    t = []\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        # GPUì—ì„œ ì‹¤í–‰ ì•ˆí•¨\n",
        "        # if torch.cuda.is_available():\n",
        "        #     targets = targets.type(torch.cuda.LongTensor)\n",
        "        #     inputs = inputs.cuda()\n",
        "\n",
        "        # inputs, targets MPSë¡œ ì˜¬ë¦¬ê¸°\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        _,outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc = calculate_accuracy(outputs, targets)\n",
        "        # loss  = criterion(outputs,targets.type(torch.cuda.LongTensor))\n",
        "        # acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    num_epochs,\n",
        "                    i,\n",
        "                    len(data_loader),\n",
        "                    losses.avg,\n",
        "                    accuracies.avg))\n",
        "\n",
        "    # ëª¨ë¸ ì €ì¥\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)  # í´ë” ì—†ìœ¼ë©´ ìƒì„±\n",
        "    # torch.save(model.state_dict(),'/content/checkpoint.pt')\n",
        "    torch.save(model.state_dict(), f'{checkpoint_path}/checkpoint.pt')\n",
        "\n",
        "    return losses.avg,accuracies.avg\n",
        "\n",
        "def test(epoch,model, data_loader ,criterion):\n",
        "    print('Testing')\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    pred = []\n",
        "    true = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            # if torch.cuda.is_available():\n",
        "                # targets = targets.cuda().type(torch.cuda.FloatTensor)\n",
        "                # inputs = inputs.cuda()\n",
        "\n",
        "            #  GPu ì•ˆë˜ì„œ ë°”ê¿ˆ\n",
        "            # inputs = inputs.cpu()  # inputsë¥¼ CPUë¡œ\n",
        "            # targets = targets.cpu()  # targetsë¥¼ CPUë¡œ\n",
        "\n",
        "            # inputs, targets MPSë¡œ ì˜¬ë¦¬ê¸° (CPUë³´ë‹¤ ë¹ ë¦„)\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            #\n",
        "\n",
        "            _,outputs = model(inputs)\n",
        "            # GPu ì•ˆë˜ì„œ ë°”ê¿ˆ\n",
        "            # loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n",
        "            # acc = calculate_accuracy(outputs,targets.type(torch.cuda.LongTensor))\n",
        "            loss = torch.mean(criterion(outputs, targets))\n",
        "            acc = calculate_accuracy(outputs, targets)\n",
        "            #\n",
        "            _,p = torch.max(outputs,1) \n",
        "            # true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
        "            true += (targets.type(torch.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
        "\n",
        "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            accuracies.update(acc, inputs.size(0))\n",
        "            sys.stdout.write(\n",
        "                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
        "                    % (\n",
        "                        i,\n",
        "                        len(data_loader),\n",
        "                        losses.avg,\n",
        "                        accuracies.avg\n",
        "                        )\n",
        "                    )\n",
        "        print('\\nAccuracy {}'.format(accuracies.avg))\n",
        "    return true,pred,losses.avg,accuracies.avg\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    batch_size = targets.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(1, 1, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1, -1))\n",
        "    n_correct_elems = correct.float().sum().item()\n",
        "    return 100* n_correct_elems / batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "b8WneBZNfysN"
      },
      "outputs": [],
      "source": [
        "#Output confusion matrix   ì„±ëŠ¥ í‰ê°€\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix  #ë‚´ê°€ ì¶”ê°€í•¨\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "def print_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    print('True positive = ', cm[0][0])\n",
        "    print('False positive = ', cm[0][1])\n",
        "    print('False negative = ', cm[1][0])\n",
        "    print('True negative = ', cm[1][1])\n",
        "    print('\\n')\n",
        "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "    sn.set(font_scale=1.4) # for label size\n",
        "    sn.heatmap(df_cm, annot=True,fmt='d', annot_kws={\"size\": 16}) # font size ,fmt='d'ë¡œ ì •ìˆ˜ í‘œí˜„\n",
        "    plt.ylabel('Actual label', size = 20)\n",
        "    plt.xlabel('Predicted label', size = 20)\n",
        "    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.ylim([2, 0])\n",
        "    plt.show()\n",
        "    calculated_acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+ cm[1][1])\n",
        "    print(\"Calculated Accuracy\",calculated_acc*100)\n",
        "\n",
        "\n",
        "    y_true = (['Fake'] * sum(cm[0]) + ['Real'] * sum(cm[1]))\n",
        "    y_pred = (['Fake'] * cm[0][0] + ['Real'] * cm[0][1] +\n",
        "            ['Fake'] * cm[1][0] + ['Real'] * cm[1][1])\n",
        "\n",
        "    # ì„±ëŠ¥ ì¶œë ¥\n",
        "    print(\"ğŸ“Š Confusion Matrix:\\n\", cm)\n",
        "    print(\"\\nğŸ“ˆ Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Fake', 'Real']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fExJLjt2AtV9"
      },
      "outputs": [],
      "source": [
        "# loss ê·¸ë˜í”„\n",
        "def plot_loss(train_loss_avg,test_loss_avg,num_epochs):\n",
        "  loss_train = train_loss_avg\n",
        "  loss_val = test_loss_avg\n",
        "  print(num_epochs)\n",
        "  epochs = range(1,num_epochs+1)\n",
        "  plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "  plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "def plot_accuracy(train_accuracy,test_accuracy,num_epochs):\n",
        "  loss_train = train_accuracy\n",
        "  loss_val = test_accuracy\n",
        "  epochs = range(1,num_epochs+1)\n",
        "  plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "  plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "  plt.title('Training and Validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rUe1XrYnDdit"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "#learning rate\n",
        "lr = 1e-4              #ì‹œì‘ 1e-5#0.001\n",
        "#number of epochs \n",
        "num_epochs = 5\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr,weight_decay = 1e-5)\n",
        "\n",
        "#class_weights = torch.from_numpy(np.asarray([1,15])).type(torch.FloatTensor).cuda()\n",
        "#criterion = nn.CrossEntropyLoss(weight = class_weights).cuda()\n",
        "\n",
        "#GPUì•ˆë˜ì„œ ìˆ˜ì •\n",
        "# criterion = nn.CrossEntropyLoss().cuda()\n",
        "#criterion = nn.CrossEntropyLoss().cpu()  # CPU ì‚¬ìš© ë²„ì „\n",
        "criterion = nn.CrossEntropyLoss().to(device) #MPS ì‚¬ìš©ë²„ì „\n",
        "#\n",
        "train_loss_avg =[]\n",
        "train_accuracy = []\n",
        "test_loss_avg = []\n",
        "test_accuracy = []\n",
        "\n",
        "# ì‹œê°„ ì¸¡ì • ì‹œì‘\n",
        "start_time = time.time()\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    l, acc = train_epoch(epoch,num_epochs,train_loader,model,criterion,optimizer)\n",
        "    train_loss_avg.append(l)\n",
        "    train_accuracy.append(acc)\n",
        "    true,pred,tl,t_acc = test(epoch,model,valid_loader,criterion)\n",
        "    test_loss_avg.append(tl)\n",
        "    test_accuracy.append(t_acc)\n",
        "    \n",
        "    epoch_end_time = time.time()\n",
        "    epoch_elapsed = epoch_end_time - epoch_start_time\n",
        "    print(f\"âœ… Epoch {epoch} ì†Œìš” ì‹œê°„: {epoch_elapsed:.2f}ì´ˆ\")\n",
        "# ì‹œê°„ ì¸¡ì • ë\n",
        "end_time = time.time()\n",
        "    \n",
        "plot_loss(train_loss_avg,test_loss_avg,len(train_loss_avg))\n",
        "plot_accuracy(train_accuracy,test_accuracy,len(train_accuracy))\n",
        "print(confusion_matrix(true,pred))\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"âœ… ì „ì²´ í•™ìŠµ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_confusion_matrix(true,pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµë¥  1e-5 -> 1e-4 \n",
        "# epoch 10 -> 20\n",
        "# ì •í™•ë„ 60->80%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# raise Exception(\"ì¤‘ê°„ ì ê²€: ì´í›„ ì…€ ì‹¤í–‰ì„ ë§‰ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_input_file_path=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/ff++/train/*'\n",
        "# test_output_file_path=f'/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset/ff++/val/*'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Model with feature visualization\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True) #Residual Network CNN\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048,num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        batch_size,seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size,seq_length,2048)\n",
        "        x_lstm,_ = self.lstm(x,None)\n",
        "        return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"âœ… Using device: {device}\")\n",
        "\n",
        "# 1. ëª¨ë¸ êµ¬ì¡°ë¥¼ ë‹¤ì‹œ ì •ì˜\n",
        "model = Model(num_classes=2).to(device)\n",
        "\n",
        "# 2. checkpoint ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model.load_state_dict(torch.load(f'{checkpoint_path}/checkpoint.pt'))\n",
        "\n",
        "# 3. í‰ê°€ ëª¨ë“œ ì „í™˜\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "new_video_files =  glob.glob(f'{test_input_file_path}/*.mp4')   # ê²½ë¡œ ë³€ê²½\n",
        "# new_video_files += glob.glob(f'{test_output_file_path}/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_FAKE_Face_only_data/*.mp4')\n",
        "# video_files += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n",
        "random.shuffle(new_video_files)\n",
        "random.shuffle(new_video_files)\n",
        "\n",
        "frame_count = []\n",
        "short_frame=[]\n",
        "\n",
        "for video_file in reversed(new_video_files): # ì´ê±° ì•ì—ì„œ ë¶€í„° í•˜ë©´ removeë¡œ ì¸í•´ frame_countë‘ video_files ê¸¸ì´ê°€ ë‹¬ë¼ì§, ê·¸ë˜ì„œ reversed ì¶”ê°€í•˜ì—¬ ë’¤ì—ì„œ ë¶€í„° íƒìƒ‰!!\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<frames):  # frames ë³€ìˆ˜ ìœ„ì—ì„œ ì¡°ì •\n",
        "    new_video_files.remove(video_file)\n",
        "    short_frame.append(video_file)\n",
        "    continue\n",
        "\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "  \n",
        "print(\"frames are \" , frame_count)\n",
        "print(\"Total no of video: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))\n",
        "print('Short_frame_count : ', len(short_frame))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "results = []\n",
        "label_list = []\n",
        "folder_path_list=[]\n",
        "base_path = '/Users/jiyeong/Desktop/ì»´ê³µ ìº¡ìŠ¤í†¤/Dataset'\n",
        "\n",
        "with torch.no_grad():\n",
        "    for video_path in tqdm(new_video_files):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_preds = []\n",
        "        frame_idx = 0\n",
        "\n",
        "        relative_path = os.path.relpath(video_path,base_path).replace(\"\\\\\", \"/\")\n",
        "        folder_path_list.append(relative_path)\n",
        "\n",
        "        # label (real/fake)\n",
        "        if 'real' in relative_path.lower():\n",
        "            label = 'REAL'\n",
        "        elif 'fake' in relative_path.lower():\n",
        "            label = 'FAKE'\n",
        "        else:\n",
        "            label = 'unknown'\n",
        "        label_list.append(label)\n",
        "        success, frame = cap.read()\n",
        "        while success:\n",
        "            frame_idx += 1\n",
        "            if frame_idx % 5 == 0:  # ë§¤ 5ë²ˆì§¸ í”„ë ˆì„ë§Œ ë½‘ì•„ì„œ ì˜ˆì¸¡ (ì†ë„ + ëŒ€í‘œì„±)\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                input_tensor = transform(frame)\n",
        "                input_tensor = input_tensor.unsqueeze(0).unsqueeze(0)  # (batch=1, seq_len=1, c=3, h, w)\n",
        "                input_tensor = input_tensor.to(device).float()\n",
        "\n",
        "                fmap, outputs = model(input_tensor)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                frame_preds.append(predicted.item())\n",
        "\n",
        "            success, frame = cap.read()\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # ë¹„ë””ì˜¤ í•˜ë‚˜ì— ëŒ€í•œ ìµœì¢… ì˜ˆì¸¡\n",
        "        if len(frame_preds) == 0:\n",
        "            final_prediction = 'Unknown'\n",
        "        else:\n",
        "            majority = round(sum(frame_preds) / len(frame_preds))  # ë‹¤ìˆ˜ê²°\n",
        "            final_prediction = 'REAL' if majority == 1 else 'FAKE'\n",
        "\n",
        "        results.append({\n",
        "            'Filename': os.path.basename(video_path),\n",
        "            'Filepath': video_path,\n",
        "            'label': label,\n",
        "            'Prediction': final_prediction\n",
        "        })\n",
        "\n",
        "# ê²°ê³¼ DataFrameìœ¼ë¡œ ë§Œë“¤ê¸°\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# ì—‘ì…€ë¡œ ì €ì¥\n",
        "output_excel_path = f'{meta_data_path}/ff+(train)_video_predictions.xlsx'\n",
        "df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"âœ… ëª¨ë“  ë¹„ë””ì˜¤ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—‘ì…€ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_excel_path}\")\n",
        "\n",
        "# Confusion Matrix ê³„ì‚°\n",
        "labels = ['REAL', 'FAKE']\n",
        "y_true = label_list  # ì‹¤ì œ ë ˆì´ë¸”\n",
        "y_pred = [result['Prediction'] for result in results]  # ì˜ˆì¸¡ ë ˆì´ë¸”\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_confusion_matrix(y_true,y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model_and_train_csv.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ba2023",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
