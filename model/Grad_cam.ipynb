{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c058b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/val/fake'\n",
    "# output_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/fake_ff'   # 영상 output 저장하는 경로\n",
    "output_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/output'   # 영상 output 저장하는 경로\n",
    "frame_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/output/fake_ff/jpg'      # jpg output 저장하는 경로\n",
    "# frame_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Grad-cam/fake_ff/jpg'      # jpg output 저장하는 경로\n",
    "predictions_file_path = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/(test)_checkpoint_1_predictions.xlsx' # 예측 후 메타 데이터\n",
    "# input_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/train/fake'    # input\n",
    "# output_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/train_fake_ff'   # 영상 output 저장하는 경로\n",
    "# frame_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Grad-cam/train_fake_ff/jpg'      # jpg output 저장하는 경로\n",
    "# predictions_file_path = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/(train)_checkpoint_1_predictions.xlsx' # 예측 후 메타 데이터\n",
    "\n",
    "checkpoint_path=f'/Users/jiyeong/HUFS.CSE.DE-fake-it/model/checkpoints'\n",
    "\n",
    "# TODO: 입력 프레임(.jpg)들이 있는 디렉토리 경로로 수정\n",
    "# input_dir = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/*'  \n",
    "input_dir = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/here/'  \n",
    "# TODO: 결과 이미지를 저장할 디렉토리 경로로 수정\n",
    "output_dir = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/there'\n",
    "output_dir_box='/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/there_box'\n",
    "base_path='/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43dece4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install face_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2acfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import face_alignment\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efded1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ MPS 디바이스 설정\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb7bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 모델 정의\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        fmap = self.model(x)\n",
    "        x = self.avgpool(fmap)\n",
    "        x = x.view(batch_size, seq_length, 2048)\n",
    "        x_lstm, _ = self.lstm(x, None)\n",
    "        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ea7f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Grad-CAM 계산 함수\n",
    "def compute_gradcam(model, input_tensor, target_class=None):\n",
    "    model.eval()\n",
    "    fmap = None\n",
    "    grad = None\n",
    "\n",
    "    def fw_hook(module, inp, out):\n",
    "        nonlocal fmap\n",
    "        fmap = out.detach()\n",
    "\n",
    "    def bw_hook(module, grad_in, grad_out):\n",
    "        nonlocal grad\n",
    "        grad = grad_out[0].detach()\n",
    "\n",
    "    last_layer = model.model[-1]\n",
    "    f = last_layer.register_forward_hook(fw_hook)\n",
    "    b = last_layer.register_backward_hook(bw_hook)\n",
    "\n",
    "    input_tensor = input_tensor.to(device).unsqueeze(0).unsqueeze(0).requires_grad_(True)\n",
    "    _, output = model(input_tensor)\n",
    "\n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    output[0, target_class].backward()\n",
    "\n",
    "    weights = grad.mean(dim=[2, 3], keepdim=True)\n",
    "    cam = (weights * fmap).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam)\n",
    "    cam = cam.squeeze().cpu().numpy()\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    cam = cv2.resize(cam, (input_tensor.shape[-1], input_tensor.shape[-2]))\n",
    "\n",
    "    f.remove()\n",
    "    b.remove()\n",
    "    return cam\n",
    "\n",
    "# ✅ MJPEG 처리 및 저장 함수\n",
    "def process_video_and_save_frames(input_video_path, output_video_path, model, frame_dir=f'{frame_path}'):\n",
    "    os.makedirs(frame_dir, exist_ok=True)\n",
    "    input_path = f'{input_video_path}/*.mp4'  #Input file path, 입력 파일 경로 - 파일 경로 수정!!\n",
    "    video_files = glob.glob(input_path)\n",
    "\n",
    "    # Ensure to use MPS for MacBook -MPS GPu 사용하기\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    # 이미 처리되어 저장된 영상 개수 확인\n",
    "    already_present_count = glob.glob(output_video_path+ '/*.mp4')\n",
    "    print(\"No of videos already present \", len(already_present_count))\n",
    "\n",
    "    # Excel 파일 로드\n",
    "    df = pd.read_excel(predictions_file_path)\n",
    "\n",
    "    top_jpg_dir = os.path.join(output_video_path, \"Top_jpg\")\n",
    "    os.makedirs(top_jpg_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    for video_file in tqdm(video_files):\n",
    "        result = str(df[df['Filepath'] == video_file]['label'].iloc[0])[0] + str(df[df['Filepath'] == video_file]['Prediction'].iloc[0])[0]\n",
    "        \n",
    "        f_name=f'({result})_'+video_file.split('/')[-1]\n",
    "        out_path = os.path.join(output_video_path,f_name) # 영상 파일 이름 추출\n",
    "        print(\"file name is : \",f_name)\n",
    "\n",
    "        file_exists = glob.glob(out_path + \"*\")\n",
    "        print(file_exists)\n",
    "        if(len(file_exists) != 0): # 이미 존재하면 pass\n",
    "            print(\"File Already exists: \" , out_path)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 고유한 파일 이름으로 저장\n",
    "        filename = os.path.basename(video_file)\n",
    "        name, _ = os.path.splitext(filename)\n",
    "        output_path = os.path.join(output_video_path, f\"({result})_{name}.mp4\")\n",
    "\n",
    "        # MP4로 저장\n",
    "        out = cv2.VideoWriter(output_path,cv2.VideoWriter_fourcc('M','J','P','G'), fps, (w, h))\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize((224, 224)),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        frame_count = 0\n",
    "\n",
    "        frame_scores = []\n",
    "        frame_images = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            original = frame.copy()\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = transform(img).to(device)\n",
    "\n",
    "            cam = compute_gradcam(model, img)\n",
    "\n",
    "            score = float(np.mean(cam))  # ✅ 전체 cam의 평균값을 점수로 사용\n",
    "\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "            heatmap = cv2.resize(heatmap, (original.shape[1], original.shape[0]))\n",
    "            overlay = 0.4 * heatmap + 0.6 * original\n",
    "            overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
    "\n",
    "            frame_scores.append((frame_count, score))\n",
    "            frame_images.append((frame_count, overlay))  # Grad-CAM 오버레이 결과 저장\n",
    "\n",
    "            # # out.write(overlay)\n",
    "\n",
    "            # # 프레임 저장\n",
    "            # frame_path = os.path.join(frame_dir, f\"({result})_{name}_{frame_count:04d}.jpg\")\n",
    "            # cv2.imwrite(frame_path, overlay)\n",
    "            frame_count += 1\n",
    "        \n",
    "        # ✅ 가장 점수가 높은 상위 10개 프레임 저장\n",
    "        frame_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_10_indices = set([idx for idx, score in frame_scores[:10]])\n",
    "\n",
    "        for idx, img in frame_images:\n",
    "            if idx in top_10_indices:\n",
    "                top_frame_path = os.path.join(top_jpg_dir, f\"({result})_{name}_TOP{idx:04d}.jpg\")\n",
    "                cv2.imwrite(top_frame_path, img)\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        # print(f\"✅ Grad-CAM 영상 저장 완료: {output_video_path}\")\n",
    "        # print(f\"✅ 프레임 이미지 {frame_count}개 저장됨: {frame_dir}/frame_XXXX.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c6c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=2)\n",
    "model.load_state_dict(torch.load(f\"{checkpoint_path}/checkpoint_1.pt\", map_location=device))\n",
    "\n",
    "process_video_and_save_frames(\n",
    "    input_video_path,\n",
    "    output_video_path,\n",
    "    model=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7337b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f91b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/nm/26yk90l550xc1ln38d9jn04r0000gn/T/ipykernel_48709/3143362613.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{checkpoint_path}/checkpoint_1.pt\", map_location=device))\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "  4%|▍         | 3/80 [00:00<00:19,  4.04it/s]/var/folders/nm/26yk90l550xc1ln38d9jn04r0000gn/T/ipykernel_48709/3143362613.py:8: RuntimeWarning: Mean of empty slice.\n",
      "  mean_val = float(patch.mean())\n",
      "/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 80/80 [00:10<00:00,  7.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_bbox(pts):\n",
    "    x, y = pts[:,0], pts[:,1]\n",
    "    return int(x.min()), int(y.min()), int(x.max()), int(y.max())\n",
    "\n",
    "def roi_activation(cam, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    patch = cam[y1:y2, x1:x2]\n",
    "    mean_val = float(patch.mean())\n",
    "\n",
    "    if np.isnan(mean_val):\n",
    "        return -1\n",
    "    return mean_val\n",
    "\n",
    "\n",
    "# TODO: 모델 클래스와 일치하도록 num_classes 설정\n",
    "model = Model(num_classes=2)\n",
    "model.load_state_dict(torch.load(f\"{checkpoint_path}/checkpoint_1.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "fa = face_alignment.FaceAlignment(\n",
    "    face_alignment.LandmarksType.TWO_D,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir_box, exist_ok=True)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ 배치 처리 루프\n",
    "# ------------------------------\n",
    "\n",
    "result=[]\n",
    "videos=glob.glob(os.path.join(input_dir, '*.jpg'))\n",
    "for img_path in tqdm(videos):\n",
    "    original = cv2.imread(img_path)\n",
    "    rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    landmarks = fa.get_landmarks(rgb)\n",
    "    if not landmarks:\n",
    "        # 얼굴이 감지되지 않으면 패스\n",
    "        continue\n",
    "    lm = landmarks[0]\n",
    "\n",
    "    # 부위별 박스 계산 (dlib/face_alignment 68점 기준)\n",
    "    jl_bbox = get_bbox(lm[0:17])\n",
    "    le_bbox = get_bbox(lm[36:42])\n",
    "    re_bbox = get_bbox(lm[42:48])\n",
    "    le_brow_bbox = get_bbox(lm[17:22])\n",
    "    re_brow_bbox = get_bbox(lm[22:27])\n",
    "    nose_bbox = get_bbox(lm[27:36])\n",
    "    mouth_bbox = get_bbox(lm[48:68])\n",
    "\n",
    "    # Grad-CAM 계산\n",
    "    img_tensor = transform(rgb).to(device)\n",
    "    cam = compute_gradcam(model, img_tensor)\n",
    "    cam = cv2.resize(cam, (original.shape[1], original.shape[0]))\n",
    "\n",
    "    # 부위별 활성도 채점\n",
    "    scores = {\n",
    "        'jawline':  roi_activation(cam, jl_bbox ),\n",
    "        'left_eye':  roi_activation(cam, le_bbox),\n",
    "        'right_eye': roi_activation(cam, re_bbox),\n",
    "        'left_eye_brow': roi_activation(cam, le_brow_bbox),\n",
    "        'right_eye_brow': roi_activation(cam, re_brow_bbox),\n",
    "        'nose':      roi_activation(cam, nose_bbox),\n",
    "        'mouth':     roi_activation(cam, mouth_bbox)\n",
    "    }\n",
    "    # most_activated = max(scores, key=scores.get)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)# 점수 정렬(null은 맨 뒤로)\n",
    "    first_activated = sorted_scores[0][0]   # 가장 큰 key\n",
    "    second_activated = sorted_scores[1][0]\n",
    "    # 최고 활성 부위 박스 가져오기\n",
    "    bbox_map = {\n",
    "        'jawline': jl_bbox,\n",
    "        'left_eye':  le_bbox,\n",
    "        'right_eye': re_bbox,\n",
    "        'left_eye_brow':le_brow_bbox,\n",
    "        'right_eye_brow':re_brow_bbox,\n",
    "        'nose':      nose_bbox,\n",
    "        'mouth':     mouth_bbox\n",
    "    }\n",
    "    f_x1, f_y1, f_x2, f_y2 = bbox_map[first_activated]\n",
    "    s_x1, s_y1, s_x2, s_y2 = bbox_map[second_activated]\n",
    "\n",
    "    # 박스 안친 결과 저장\n",
    "    out_path = os.path.join(output_dir, os.path.basename(img_path))\n",
    "    cv2.imwrite(out_path, original)\n",
    "\n",
    "    # 결과 오버레이\n",
    "    cv2.rectangle(original, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)  # Green\n",
    "    # cv2.putText(\n",
    "    #     original,\n",
    "    #     first_activated,\n",
    "    #     (f_x1, f_y1 - 10),\n",
    "    #     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    #     0.8,\n",
    "    #     (0, 255, 0),\n",
    "    #     2\n",
    "    # )\n",
    "\n",
    "    cv2.rectangle(original, (s_x1, s_y1), (s_x2, s_y2), (255,0, 0), 2)  # Blue\n",
    "    # cv2.putText(\n",
    "    #     original,\n",
    "    #     second_activated,\n",
    "    #     (s_x1, s_y1 - 10),\n",
    "    #     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    #     0.8,\n",
    "    #     (0, 255, 0),\n",
    "    #     2\n",
    "    # )\n",
    "\n",
    "    result.append({\n",
    "        'file_name': os.path.basename(img_path),\n",
    "        'first_activate': first_activated,\n",
    "        'second_activate': second_activated,\n",
    "        'f_x1': f_x1, 'f_y1': f_y1, 'f_x2': f_x2, 'f_y2': f_y2,\n",
    "        's_x1': s_x1, 's_y1': s_y1, 's_x2': s_x2, 's_y2': s_y2,\n",
    "        'jawline': scores['jawline'],\n",
    "        'left_eye': scores['left_eye'],\n",
    "        'right_eye': scores['right_eye'],\n",
    "        'left_eye_brow': scores['left_eye_brow'],\n",
    "        'right_eye_brow': scores['right_eye_brow'],\n",
    "        'nose': scores['nose'],\n",
    "        'mouth': scores['mouth']\n",
    "    })\n",
    "\n",
    "    # 박스 친 결과 저장\n",
    "    out_path_box = os.path.join(output_dir_box, os.path.basename(img_path))\n",
    "    cv2.imwrite(out_path_box, original)\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df = df[[\n",
    "    'file_name', 'first_activate', 'second_activate',\n",
    "    'f_x1', 'f_y1', 'f_x2', 'f_y2',\n",
    "    's_x1', 's_y1', 's_x2', 's_y2',\n",
    "    'jawline','left_eye','right_eye',\n",
    "    'left_eye_brow','right_eye_brow',\n",
    "    'nose','mouth'\n",
    "]]\n",
    "\n",
    "# 엑셀 저장\n",
    "df.to_excel(f\"{base_path}/activation_results.xlsx\", index=False)\n",
    "\n",
    "# JSON 저장\n",
    "df.to_json(f\"{base_path}/activation_results.json\", orient='records', force_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
