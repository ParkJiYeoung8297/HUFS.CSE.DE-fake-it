{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/val/fake'\n",
    "# output_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam(Top)'   # 영상 output 저장하는 경로\n",
    "# frame_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam(Top)'      # jpg output 저장하는 경로\n",
    "# predictions_file_path = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/(test)_checkpoint_1_predictions.xlsx' # 예측 후 메타 데이터\n",
    "\n",
    "input_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/test/fake/FaceSwap'    # input\n",
    "output_video_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/Grad-cam'   # 영상 output 저장하는 경로\n",
    "frame_path=f'/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/frame'      # jpg output 저장하는 경로\n",
    "predictions_file_path = '/Users/jiyeong/HUFS.CSE.DE-fake-it/model/checkpoints/(test)_checkpoint1_predictions.xlsx' # 예측 후 메타 데이터\n",
    "\n",
    "checkpoint_path=f'/Users/jiyeong/HUFS.CSE.DE-fake-it/model/checkpoints'\n",
    "checkpoint_name='checkpoint1'\n",
    "\n",
    "# # TODO: 입력 프레임(.jpg)들이 있는 디렉토리 경로로 수정\n",
    "# # input_dir = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/Grad-cam/*'  \n",
    "# input_dir = '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/Grad-cam/Top_jpg/'  \n",
    "# # TODO: 결과 이미지를 저장할 디렉토리 경로로 수정\n",
    "# output_dir_box='/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/Grad-cam/there_boxxxx'\n",
    "# base_path='/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/ff++/Grad-cam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43dece4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install face_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2acfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import face_alignment\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efded1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ✅ MPS 디바이스 설정\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else (\n",
    "torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb7bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 모델 정의\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        fmap = self.model(x)\n",
    "        x = self.avgpool(fmap)\n",
    "        x = x.view(batch_size, seq_length, 2048)\n",
    "        x_lstm, _ = self.lstm(x, None)\n",
    "        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ea7f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Grad-CAM 계산 함수\n",
    "def compute_gradcam(model, input_tensor, target_class=None):\n",
    "    model.eval()\n",
    "    fmap = None\n",
    "    grad = None\n",
    "\n",
    "    def fw_hook(module, inp, out):\n",
    "        nonlocal fmap\n",
    "        fmap = out.detach()\n",
    "\n",
    "    def bw_hook(module, grad_in, grad_out):\n",
    "        nonlocal grad\n",
    "        grad = grad_out[0].detach()\n",
    "\n",
    "    last_layer = model.model[-1]\n",
    "    f = last_layer.register_forward_hook(fw_hook)\n",
    "    b = last_layer.register_backward_hook(bw_hook)\n",
    "\n",
    "    input_tensor = input_tensor.to(device).unsqueeze(0).unsqueeze(0).requires_grad_(True)\n",
    "    _, output = model(input_tensor)\n",
    "\n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    output[0, target_class].backward()\n",
    "\n",
    "    weights = grad.mean(dim=[2, 3], keepdim=True)\n",
    "    cam = (weights * fmap).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam)\n",
    "    cam = cam.squeeze().cpu().numpy()\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    cam = cv2.resize(cam, (input_tensor.shape[-1], input_tensor.shape[-2]))\n",
    "\n",
    "    f.remove()\n",
    "    b.remove()\n",
    "    return cam\n",
    "\n",
    "# ✅ MJPEG 처리 및 저장 함수\n",
    "def process_video_and_save_frames(input_video_path, output_video_path, model, frame_dir=f'{frame_path}'):\n",
    "    # 0. Set up\n",
    "    # 1). 장치 확인\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 2). 이미 처리되어 저장된 영상 개수 확인\n",
    "    input_path = f'{input_video_path}/*.mp4'\n",
    "    video_files = glob.glob(input_path)\n",
    "    already_present_count = glob.glob(output_video_path+ '/*.mp4')\n",
    "    print(\"No of videos already present \", len(already_present_count))\n",
    "\n",
    "    # 3) ouput 폴더 만들기\n",
    "    os.makedirs(frame_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_path, exist_ok=True)\n",
    "    top_jpg_dir = os.path.join(output_video_path, \"Top_jpg\")\n",
    "    os.makedirs(top_jpg_dir, exist_ok=True)\n",
    "\n",
    "    # 4) REAL/FAKE 라벨 불러오기\n",
    "    df = pd.read_excel(predictions_file_path)\n",
    "\n",
    "    for video_file in tqdm(video_files):\n",
    "        result = str(df[df['Filepath'] == video_file]['label'].iloc[0])[0] + str(df[df['Filepath'] == video_file]['Prediction'].iloc[0])[0]\n",
    "        f_name=f'({result})_'+video_file.split('/')[-1]\n",
    "        out_path = os.path.join(output_video_path,f_name) # 영상 파일 이름 추출\n",
    "        file_exists = glob.glob(out_path + \"*\")\n",
    "        if(len(file_exists) != 0): # 이미 존재하면 pass\n",
    "            print(\"File Already exists: \" , out_path)\n",
    "            continue\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # 5) ouput MP4 파일 이름 지정\n",
    "        filename = os.path.basename(video_file)\n",
    "        name, _ = os.path.splitext(filename)\n",
    "        output_path = os.path.join(output_video_path, f\"({result})_{name}.mp4\")\n",
    "        out = cv2.VideoWriter(output_path,cv2.VideoWriter_fourcc('M','J','P','G'), fps, (w, h))\n",
    "\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((112, 112)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        frame_count = 0\n",
    "\n",
    "        frame_scores = []\n",
    "        frame_images = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            original = frame.copy()\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = transform(img).to(device)\n",
    "\n",
    "            cam = compute_gradcam(model, img)\n",
    "\n",
    "            score = float(np.mean(cam))  # ✅ 전체 cam의 평균값을 점수로 사용\n",
    "\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "            heatmap = cv2.resize(heatmap, (original.shape[1], original.shape[0]))\n",
    "            overlay = 0.4 * heatmap + 0.6 * original\n",
    "            overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
    "\n",
    "            frame_scores.append((frame_count, score))\n",
    "            frame_images.append((frame_count, overlay))  # Grad-CAM 오버레이 결과 저장\n",
    "\n",
    "            out.write(overlay) # 오버레이 프레임 저장\n",
    "\n",
    "            # 프레임 저장\n",
    "            frame_path = os.path.join(frame_dir, f\"({result})_{name}_{frame_count:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, overlay)\n",
    "            frame_count += 1\n",
    "        \n",
    "        # ✅ 가장 점수가 높은 상위 10개 프레임 저장\n",
    "        frame_scores.sort(key=lambda x: x[1], reverse=True)  # 점수 기준으로 정렬\n",
    "        top_10_indices = [idx for idx, score in frame_scores[:10]]  # 상위 10개 인덱스 추출\n",
    "\n",
    "        for rank, idx in enumerate(top_10_indices):\n",
    "            # 순위(rank)는 상위 10개 인덱스에 대한 순서입니다.\n",
    "            img = frame_images[idx][1]  # top_10_indices에 해당하는 프레임 이미지를 가져옵니다.\n",
    "            score = frame_scores[rank][1]  # 해당 프레임의 점수\n",
    "\n",
    "            top_frame_path = os.path.join(top_jpg_dir, f\"{f_name}_TOP{rank + 1}_Score{score:.4f}.jpg\")\n",
    "            cv2.imwrite(top_frame_path, img)   # 상위 10개 프레임 저장\n",
    "\n",
    "\n",
    "        cap.release()  # 모든 프레임 저장\n",
    "        out.release()  # mp4 저장\n",
    "        print(f\"✅ Grad-CAM 영상 저장 완료: {output_video_path}\")\n",
    "        print(f\"✅ 프레임 이미지 {frame_count}개 저장됨: {frame_dir}/frame_XXXX.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfb49c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nm/26yk90l550xc1ln38d9jn04r0000gn/T/ipykernel_961/3082594020.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{checkpoint_path}/{checkpoint_name}.pt\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "No of videos already present  0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/(test)_checkpoint1_predictions.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mprocess_video_and_save_frames\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_video_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m, in \u001b[0;36mprocess_video_and_save_frames\u001b[0;34m(input_video_path, output_video_path, model, frame_dir)\u001b[0m\n\u001b[1;32m     56\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(top_jpg_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 4) REAL/FAKE 라벨 불러오기\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m tqdm(video_files):\n\u001b[1;32m     62\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilepath\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m video_file][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilepath\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m video_file][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ba2023/lib/python3.8/site-packages/pandas/io/excel/_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ba2023/lib/python3.8/site-packages/pandas/io/excel/_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1496\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1503\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/ba2023/lib/python3.8/site-packages/pandas/io/excel/_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1369\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1374\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1375\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ba2023/lib/python3.8/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jiyeong/Desktop/컴공 캡스톤/Dataset/(test)_checkpoint1_predictions.xlsx'"
     ]
    }
   ],
   "source": [
    "model = Model(num_classes=2)\n",
    "model.load_state_dict(torch.load(f\"{checkpoint_path}/{checkpoint_name}.pt\", map_location=device))\n",
    "\n",
    "process_video_and_save_frames(\n",
    "    input_video_path,\n",
    "    output_video_path,\n",
    "    model=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7337b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/nm/26yk90l550xc1ln38d9jn04r0000gn/T/ipykernel_45950/3133928690.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{checkpoint_path}/checkpoint_1.pt\", map_location=device))\n",
      "  0%|          | 0/9950 [00:00<?, ?it/s]/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "  0%|          | 11/9950 [00:01<17:01,  9.73it/s]/var/folders/nm/26yk90l550xc1ln38d9jn04r0000gn/T/ipykernel_45950/3133928690.py:8: RuntimeWarning: Mean of empty slice.\n",
      "  mean_val = float(patch.mean())\n",
      "/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|          | 23/9950 [00:02<15:40, 10.55it/s]/Users/jiyeong/anaconda3/envs/ba2023/lib/python3.8/site-packages/face_alignment/api.py:147: UserWarning: No faces were detected.\n",
      "  warnings.warn(\"No faces were detected.\")\n",
      "100%|██████████| 9950/9950 [14:37<00:00, 11.34it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_bbox(pts):\n",
    "    x, y = pts[:,0], pts[:,1]\n",
    "    return int(x.min()), int(y.min()), int(x.max()), int(y.max())\n",
    "\n",
    "def roi_activation(cam, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    patch = cam[y1:y2, x1:x2]\n",
    "    mean_val = float(patch.mean())\n",
    "\n",
    "    if np.isnan(mean_val):\n",
    "        return -1\n",
    "    return mean_val\n",
    "\n",
    "# ✅ 추가 함수 정의\n",
    "def get_gradcam_peak(cam):\n",
    "    y, x = np.unravel_index(np.argmax(cam), cam.shape)\n",
    "    return x, y\n",
    "\n",
    "def is_point_in_bbox(x, y, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return x1 <= x <= x2 and y1 <= y <= y2\n",
    "\n",
    "def get_center(bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "def find_roi_by_distance(cam, bbox_dict):\n",
    "    peak_x, peak_y = get_gradcam_peak(cam)\n",
    "\n",
    "    distances = []\n",
    "    for region, bbox in bbox_dict.items():\n",
    "        cx, cy = get_center(bbox)\n",
    "        dist = ((peak_x - cx) ** 2 + (peak_y - cy) ** 2) ** 0.5\n",
    "        distances.append((region, dist))\n",
    "\n",
    "    # 거리순으로 정렬\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    closest_roi = distances[0][0]\n",
    "    second_closest_roi = distances[1][0]\n",
    "\n",
    "    return closest_roi, second_closest_roi, (peak_x, peak_y)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: 모델 클래스와 일치하도록 num_classes 설정\n",
    "model = Model(num_classes=2)\n",
    "model.load_state_dict(torch.load(f\"{checkpoint_path}/{checkpoint_name}.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "fa = face_alignment.FaceAlignment(\n",
    "    face_alignment.LandmarksType.TWO_D,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "os.makedirs(output_dir_box, exist_ok=True)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ 배치 처리 루프\n",
    "# ------------------------------\n",
    "\n",
    "result=[]\n",
    "videos=glob.glob(os.path.join(input_dir, '*.jpg'))\n",
    "for img_path in tqdm(videos):\n",
    "    original = cv2.imread(img_path)\n",
    "    rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    landmarks = fa.get_landmarks(rgb)\n",
    "    if not landmarks:\n",
    "        # 얼굴이 감지되지 않으면 패스\n",
    "        continue\n",
    "    lm = landmarks[0]\n",
    "\n",
    "    # 부위별 박스 계산 (dlib/face_alignment 68점 기준)\n",
    "    jl_bbox = get_bbox(lm[0:17])\n",
    "    le_bbox = get_bbox(lm[36:42])\n",
    "    re_bbox = get_bbox(lm[42:48])\n",
    "    le_brow_bbox = get_bbox(lm[17:22])\n",
    "    re_brow_bbox = get_bbox(lm[22:27])\n",
    "    nose_bbox = get_bbox(lm[27:36])\n",
    "    mouth_bbox = get_bbox(lm[48:68])\n",
    "\n",
    "    # Grad-CAM 계산\n",
    "    img_tensor = transform(rgb).to(device)\n",
    "    cam = compute_gradcam(model, img_tensor)\n",
    "    cam = cv2.resize(cam, (original.shape[1], original.shape[0]))\n",
    "\n",
    "    # 부위별 활성도 채점\n",
    "    scores = {\n",
    "        'jawline':  roi_activation(cam, jl_bbox ),\n",
    "        'left_eye':  roi_activation(cam, le_bbox),\n",
    "        'right_eye': roi_activation(cam, re_bbox),\n",
    "        'left_eye_brow': roi_activation(cam, le_brow_bbox),\n",
    "        'right_eye_brow': roi_activation(cam, re_brow_bbox),\n",
    "        'nose':      roi_activation(cam, nose_bbox),\n",
    "        'mouth':     roi_activation(cam, mouth_bbox)\n",
    "    }\n",
    "    # most_activated = max(scores, key=scores.get)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)# 점수 정렬(null은 맨 뒤로)\n",
    "    first_activated = sorted_scores[0][0]   # 가장 큰 key\n",
    "    second_activated = sorted_scores[1][0]\n",
    "    # 최고 활성 부위 박스 가져오기\n",
    "    bbox_map = {\n",
    "        'jawline': jl_bbox,\n",
    "        'left_eye':  le_bbox,\n",
    "        'right_eye': re_bbox,\n",
    "        'left_eye_brow':le_brow_bbox,\n",
    "        'right_eye_brow':re_brow_bbox,\n",
    "        'nose':      nose_bbox,\n",
    "        'mouth':     mouth_bbox\n",
    "    }\n",
    "    f_x1, f_y1, f_x2, f_y2 = bbox_map[first_activated]\n",
    "    s_x1, s_y1, s_x2, s_y2 = bbox_map[second_activated]\n",
    "\n",
    "    # Grad-CAM peak 좌표 및 가장 가까운 ROI 두 개\n",
    "    peak_roi, second_peak_roi, (px, py) = find_roi_by_distance(cam, bbox_map)\n",
    "\n",
    "    # peak가 어디 순위에 있는지 판단\n",
    "    if peak_roi == first_activated:\n",
    "        peak_rank = 1\n",
    "    elif peak_roi == second_activated:\n",
    "        peak_rank = 2\n",
    "    else:\n",
    "        peak_rank = -1\n",
    "    \n",
    "    # rank 판단은 동일하게 유지\n",
    "    if second_peak_roi == first_activated:\n",
    "        second_peak_rank = 1\n",
    "    elif second_peak_roi == second_activated:\n",
    "        second_peak_rank = 2\n",
    "    else:\n",
    "        second_peak_rank = -1\n",
    "\n",
    "\n",
    "    # # 결과 오버레이\n",
    "    # cv2.rectangle(original, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)  # Green\n",
    "    # # cv2.putText(\n",
    "    # #     original,\n",
    "    # #     first_activated,\n",
    "    # #     (f_x1, f_y1 - 10),\n",
    "    # #     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    # #     0.8,\n",
    "    # #     (0, 255, 0),\n",
    "    # #     2\n",
    "    # # )\n",
    "\n",
    "    # cv2.rectangle(original, (s_x1, s_y1), (s_x2, s_y2), (255,0, 0), 2)  # Blue\n",
    "    # # cv2.putText(\n",
    "    # #     original,\n",
    "    # #     second_activated,\n",
    "    # #     (s_x1, s_y1 - 10),\n",
    "    # #     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    # #     0.8,\n",
    "    # #     (0, 255, 0),\n",
    "    # #     2\n",
    "    # # )\n",
    "\n",
    "    result.append({\n",
    "        'file_name': os.path.basename(img_path),\n",
    "        'first_activate': first_activated,\n",
    "        'second_activate': second_activated,\n",
    "        'f_x1': f_x1, 'f_y1': f_y1, 'f_x2': f_x2, 'f_y2': f_y2,\n",
    "        's_x1': s_x1, 's_y1': s_y1, 's_x2': s_x2, 's_y2': s_y2,\n",
    "        'jawline': scores['jawline'],\n",
    "        'left_eye': scores['left_eye'],\n",
    "        'right_eye': scores['right_eye'],\n",
    "        'left_eye_brow': scores['left_eye_brow'],\n",
    "        'right_eye_brow': scores['right_eye_brow'],\n",
    "        'nose': scores['nose'],\n",
    "        'mouth': scores['mouth'],\n",
    "        'gradcam_peak_x': px,\n",
    "        'gradcam_peak_y': py,\n",
    "        'peak_roi': peak_roi,\n",
    "        'peak_rank': peak_rank,\n",
    "        'peak_roi_2nd': second_peak_roi,\n",
    "        'peak_rank_2nd': second_peak_rank\n",
    "\n",
    "    })\n",
    "\n",
    "    # 박스 친 결과 저장\n",
    "    # out_path_box = os.path.join(output_dir_box, os.path.basename(img_path))\n",
    "    # cv2.imwrite(out_path_box, original)\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df = df[[\n",
    "    'file_name', 'first_activate', 'second_activate',\n",
    "     'peak_roi', 'peak_rank','peak_roi_2nd', 'peak_rank_2nd',\n",
    "    'f_x1', 'f_y1', 'f_x2', 'f_y2',\n",
    "    's_x1', 's_y1', 's_x2', 's_y2',\n",
    "    'jawline','left_eye','right_eye',\n",
    "    'left_eye_brow','right_eye_brow',\n",
    "    'nose','mouth',\n",
    "    'gradcam_peak_x', 'gradcam_peak_y'\n",
    "]]\n",
    "\n",
    "# 엑셀 저장\n",
    "df.to_excel(f\"{base_path}/activation_results2.xlsx\", index=False)\n",
    "\n",
    "# JSON 저장\n",
    "df.to_json(f\"{base_path}/activation_results2.json\", orient='records', force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2adcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
