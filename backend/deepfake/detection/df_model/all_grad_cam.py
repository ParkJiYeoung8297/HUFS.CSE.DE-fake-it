import cv2
import torch
import torchvision.transforms as T
import numpy as np
import torch.nn.functional as F
from torchvision import models
from torch import nn
import os
import glob
import pandas as pd
import face_alignment
from tqdm import tqdm
from django.conf import settings
from pathlib import Path
import subprocess
import cv2
import json
import requests
from .model import Model

checkpoint_path=Path(__file__).resolve().parent

# âœ… Set the device to MPS(for Mac) if available, otherwise fallback to CUDA or CPU
device = torch.device("mps") if torch.backends.mps.is_available() else (
torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
)
print(f"Using device: {device}")




# âœ… Grad-CAM computation for binary classification
def compute_gradcam_binary(model, input_tensor, target_class=0,device = torch.device("mps")):
    fmap = None
    grad = None

    def fw_hook(module, inp, out):
        nonlocal fmap
        fmap = out.detach()

    def bw_hook(module, grad_in, grad_out):
        nonlocal grad
        grad = grad_out[0].detach()

    last_layer = model.model[-1]
    f = last_layer.register_forward_hook(fw_hook)
    b = last_layer.register_backward_hook(bw_hook)

    input_tensor = input_tensor.to(device).unsqueeze(0).unsqueeze(0).requires_grad_(True)
    _, binary_output, method_output = model(input_tensor)

    # Get the probability of the target class
    prob = F.softmax(binary_output, dim=1)[0, target_class].item()

    # Predict binary and method classes
    binary_pred = torch.argmax(binary_output, dim=1).item()   # 0: fake, 1: real
    method_pred = torch.argmax(method_output, dim=1).item()   # 0: original, 1~6: fake methods, 7: others

    # ğŸ”´ Condition 1: If the prediction is real and the method is original, skip CAM computation / ì¡°ê±´ 1: real(1) + original(0) â†’ CAM X
    if binary_pred == 1 and method_pred==0:
        cam = np.zeros((input_tensor.shape[-2], input_tensor.shape[-1]))
        f.remove()
        b.remove()
        return cam,prob,binary_pred, method_pred

    # Grad-CAM for fake class (target_class = 0)
    target_class = 0
    model.zero_grad()
    binary_output[0, target_class].backward()

    # Compute Grad-CAM
    weights = grad.mean(dim=[2, 3], keepdim=True)
    cam = (weights * fmap).sum(dim=1, keepdim=True)
    cam = F.relu(cam).squeeze().cpu().numpy()

    # ğŸ”µ Condition 2: If the prediction is fake and the method is not original, enhance CAM / ì¡°ê±´ 2: fake (0) + method (1~7) (â‰  0) â†’ CAM â†‘
    if binary_pred == 0 and method_pred != 0:
        cam *=1.5

    # Normalize and resize the CAM
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
    cam = cv2.resize(cam, (input_tensor.shape[-1], input_tensor.shape[-2]))



    f.remove()
    b.remove()
    return cam, prob, binary_pred, method_pred

def get_bbox(pts):
    x, y = pts[:,0], pts[:,1]
    return int(x.min()), int(y.min()), int(x.max()), int(y.max())

def roi_activation(cam, bbox):
    x1, y1, x2, y2 = bbox
    patch = cam[y1:y2, x1:x2]
    mean_val = float(patch.mean())

    if np.isnan(mean_val):
        return -1
    return mean_val

def analyze_roi_activation(video_dir,file_name, result,model):

    fa = face_alignment.FaceAlignment(
        face_alignment.LandmarksType.TWO_D,
        device=str(device)
    )

    os.makedirs(video_dir, exist_ok=True)

    transform = T.Compose([
        T.ToTensor(),
        T.Resize((224, 224)),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])

    roi_result = []
    roi_analyze_result={}

    # video_paths = glob.glob(os.path.join(video_path, '*.mp4'))

#   for video_path in tqdm(video_paths):

    facial_region=['jawline', 'left_eye', 'right_eye', 'left_eye_brow', 'right_eye_brow', 'nose', 'mouth','None']
    first_detection_count = {key: 0 for key in facial_region}
    second_detection_count = {key: 0 for key in facial_region}
    detection_probabillity={key: 0.0 for key in facial_region}


    video_path_file_name=os.path.join(video_dir,file_name)
    cap = cv2.VideoCapture(video_path_file_name)
    frame_idx = 0
    # video_name = os.path.splitext(os.path.basename(video_path))[0]


    # â–¶ï¸ ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # ë°•ìŠ¤ë§Œ ê·¸ë¦° ì˜ìƒ
    video_writer_box = cv2.VideoWriter(
        os.path.join(video_dir, "output_box_on_original.mp4"),
        fourcc, fps, (width, height)
    )

    # Grad-CAM + ë°•ìŠ¤ ì˜ìƒ
    video_writer_cam = cv2.VideoWriter(
        os.path.join(video_dir, f"grad_cam_on_original.mp4"),
        fourcc, fps, (width, height)
    )

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        landmarks = fa.get_landmarks(rgb)
        if not landmarks:
            frame_idx += 1
            continue
        lm = landmarks[0]

        # ROI BBoxes
        bbox_map = {
            'jawline': get_bbox(lm[0:17]),
            'left_eye': get_bbox(lm[36:42]),
            'right_eye': get_bbox(lm[42:48]),
            'left_eye_brow': get_bbox(lm[17:22]),
            'right_eye_brow': get_bbox(lm[22:27]),
            'nose': get_bbox(lm[27:36]),
            'mouth': get_bbox(lm[48:68]),
        }

        # Grad-CAM
        img = transform(rgb).to(device)
        cam, cam_score, binary_pred, method_pred = compute_gradcam_binary(model, img)
        cam = cv2.resize(cam, (frame.shape[1], frame.shape[0]))

        scores = {region: roi_activation(cam, box) for region, box in bbox_map.items()}
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        first_activated_region = sorted_scores[0][0]
        second_activated_region = sorted_scores[1][0]

        f_x1, f_y1, f_x2, f_y2 = bbox_map[first_activated_region]
        s_x1, s_y1, s_x2, s_y2 = bbox_map[second_activated_region]

        # 1ï¸âƒ£ Grad-CAM íˆíŠ¸ë§µ ì˜¤ë²„ë ˆì´
        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
        overlay = cv2.addWeighted(frame, 0.6, heatmap, 0.4, 0)

        # 2ï¸âƒ£ ì›ë³¸ í”„ë ˆì„ ë³µì‚¬í•´ì„œ ë°•ìŠ¤ìš© ì¤€ë¹„
        frame_with_box = frame.copy()
        overlay_with_box = overlay.copy()

        # 3ï¸âƒ£ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        if scores[first_activated_region] > 0:
            cv2.rectangle(frame_with_box, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)
            cv2.rectangle(overlay_with_box, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)
        else:
            first_activated_region="None"

        if scores[second_activated_region] > 0:
            cv2.rectangle(frame_with_box, (s_x1, s_y1), (s_x2, s_y2), (255, 0, 0), 2)
            cv2.rectangle(overlay_with_box, (s_x1, s_y1), (s_x2, s_y2), (255, 0, 0), 2)
        else:
            second_activated_region="None"

    #   # 4ï¸âƒ£ íŒŒì¼ ì €ì¥
        file_id = f"{file_name}_frame{frame_idx:04d}"
    #   cv2.imwrite(os.path.join(output_dir_box, f"{file_id}_roi.jpg"), frame_with_box)
    #   cv2.imwrite(os.path.join(output_dir_box, f"{file_id}_gradcam.jpg"), overlay_with_box)

    #  4ï¸âƒ£ ì˜ìƒ íŒŒì¼ ì €ì¥
        video_writer_box.write(frame_with_box)
        video_writer_cam.write(overlay_with_box)

        first_detection_count[first_activated_region]+=1
        second_detection_count[second_activated_region]+=1
        for key in facial_region:
            if key!="None" and scores[key]!=-1:
                detection_probabillity[key]+=cam_score * scores[key]

        roi_result.append({
            'file_name': file_id,
            'cam_score': cam_score,
            'first_activate_region': first_activated_region,
            'second_activate_region': second_activated_region,
            'f_x1': f_x1, 'f_y1': f_y1, 'f_x2': f_x2, 'f_y2': f_y2,
            's_x1': s_x1, 's_y1': s_y1, 's_x2': s_x2, 's_y2': s_y2,
            **scores,
        })

        frame_idx += 1

    cap.release()
    video_writer_box.release()
    video_writer_cam.release()

    # Printing all the dictionaries
    first_detection_rate = {key: round((value / frame_idx)*100, 2) for key, value in first_detection_count.items()}
    second_detection_rate = {key: round((value / frame_idx)*100, 2) for key, value in second_detection_count.items()}

    # ğŸ“Œ Note: A high proportion of 'None' may inflate the relative Contribution (%) and should be interpreted with caution.
    raw_detection_probabillity= {key: round(value, 4) for key, value in detection_probabillity.items()}
    probabillity_total = sum(detection_probabillity.values())
    detection_probabillity= {key: round((value/probabillity_total)*100, 2) for key, value in detection_probabillity.items()}

    # print("Video name:", file_name)
    # print("Facial Region:", facial_region)
    # print("First Detection Count:", first_detection_count)
    # print("Second Detection Count:", second_detection_count)
    # print("First Detection Rate:", first_detection_rate)
    # print("Second Detection Rate:", second_detection_rate)
    # print("Raw_Detection Probability:", raw_detection_probabillity)
    # print("Detection Probability:", detection_probabillity)

    roi_analyze_result = {
    "video_name": file_name,
    "binary_pred":result["Prediction"], 
    "cam_score":result["Probability"],
    "method_pred":result["Method"],
    "facial_region": facial_region,
    "first_detection_count": first_detection_count,
    "second_detection_count": second_detection_count,
    "first_detection_rate": first_detection_rate,
    "second_detection_rate": second_detection_rate,
    "raw_detection_probability": raw_detection_probabillity,
    "detection_probability": detection_probabillity
    }

    # í…Œì´ë¸” ë§Œë“¤ê¸°
    table_data = []
    for region in facial_region:
        row = {
            "region": region,
            "first_count": f"{first_detection_count.get(region, '0')} ({first_detection_rate.get(region, '0.00')}%)",
            "second_count": f"{second_detection_count.get(region, '0')} ({second_detection_rate.get(region, '0.00')}%)",
            "confidence": "-" if detection_probabillity.get(region) is None else f"{detection_probabillity.get(region, '0.00')}%"
        }
        table_data.append(row)

    # ë§ˆì§€ë§‰ ì´í•© í–‰ ì¶”ê°€
    table_data.append({
        "region": "total",
        "first_count": sum(int(v) for v in first_detection_count.values()),
        "second_count": sum(int(v) for v in second_detection_count.values()),
        "confidence": "100.00%"
    })


    return roi_analyze_result,table_data



def all_calculate_roi_scores(video_path,file_name,result,checkpoint_name='checkpoint_v35',selected_model='EfficientNet-b0'):
    print("ì—¬ê¸°ìš”1")
    # ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜
    model = Model(num_binary_classes=2, num_method_classes=7, model_name=selected_model).to(device)
    model.load_state_dict(torch.load(f'{checkpoint_path}/{checkpoint_name}.pt', map_location=device))
    model.eval()

    roi_analyze_result,table_data=analyze_roi_activation(video_path,file_name,result, model)

    # ì´ë¯¸ ì €ì¥ëœ ì›ë³¸ ì˜ìƒ ê²½ë¡œ
    grad_video_path_original = os.path.join(video_path, 'grad_cam_on_original.mp4')

    # ë³€í™˜ í›„ ì €ì¥í•  ê²½ë¡œ ì„¤ì •
    output_filename = "converted_grad_cam_on_original.mp4"
    output_path = os.path.join(video_path, output_filename)

    # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ ìˆ˜í–‰
    try:
        subprocess.run([
            'ffmpeg', '-i', grad_video_path_original,
            '-vcodec', 'libx264',
            '-acodec', 'aac',
            '-strict', 'experimental',
            '-y',  # ë®ì–´ì“°ê¸°
            output_path
        ], check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        print(f"âœ… Video converted: {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error occurred during conversion: {e}")

    # ì´ë¯¸ ì €ì¥ëœ ì›ë³¸ ì˜ìƒ ê²½ë¡œ
    grad_video_path_original = os.path.join(video_path, 'output_box_on_original.mp4')

    # ë³€í™˜ í›„ ì €ì¥í•  ê²½ë¡œ ì„¤ì •
    output_filename = "converted_output_box_on_original.mp4"
    output_path = os.path.join(video_path, output_filename)

    # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ ìˆ˜í–‰
    try:
        subprocess.run([
            'ffmpeg', '-i', grad_video_path_original,
            '-vcodec', 'libx264',
            '-acodec', 'aac',
            '-strict', 'experimental',
            '-y',  # ë®ì–´ì“°ê¸°
            output_path
        ], check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        print(f"âœ… Video converted: {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error occurred during conversion: {e}")


#ì—¬ê¸°ë¶€í„°
    def format_prompt(summary):
        print(f"summary: {summary}")
        prompt = (
            f" ì•„ë˜ì— ì œê³µëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•´ë‹¹ ì˜ìƒì„ REAL ë˜ëŠ” FAKEë¡œ íŒë‹¨í•œ ê·¼ê±°ë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
            f"ì‘ë‹µì€ ë¶„ì„ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³ , ë‹¤ìŒ í˜•ì‹ì²˜ëŸ¼ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”:\n"
            f"1. ...\n2. ...\n3. ...\n"
            f"ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼:ì´ ëª¨ë¸ì€ REAL/FAKE íŒë‹¨ì—ì„œ {summary['binary_pred']}ë¡œ ì˜ˆì¸¡í–ˆìœ¼ë©°, í™•ë¥ ì€ {summary['cam_score']}ì…ë‹ˆë‹¤. ë”¥í˜ì´í¬ ê¸°ë²• ë¶„ë¥˜ ê²°ê³¼: {summary['method_pred']}"
            f"ì°¸ê³  : originalì€ ìœ„ì¡° í”ì ì´ ì—†ëŠ” ì›ë³¸ ì˜ìƒ, othersëŠ” FaceForensics++ì˜ 5ê°€ì§€ ê¸°ë²• ì™¸ì˜ ìœ„ì¡° ë°©ì‹ì…ë‹ˆë‹¤."
            f"ì•„ë˜ëŠ” ì˜ìƒ {summary['video_name']}ì— ëŒ€í•œ Grad-CAM ê¸°ë°˜ ROI í™œì„±ë„ ë¶„ì„ ìš”ì•½ì…ë‹ˆë‹¤."

            f"ë¶„ì„ ë°ì´í„° ì„¤ëª… (ëª¨ë“  ê°’ì€ ì˜ìƒ ì „ì²´ í”„ë ˆì„ì„ í†µí•©í•œ í†µê³„ ê¸°ë°˜ì…ë‹ˆë‹¤):\n"
            f" - ê°€ì§œì— ì˜í–¥ì„ ì£¼ëŠ” ë¶€ë¶„ì„ grad-camìœ¼ë¡œ ì‹œê°í™”í•˜ì—¬, ì˜ìƒ ë‚´ì— ëª¨ë“  í”„ë ˆì„ì„ í†µí•©í•œ ê°’ì…ë‹ˆë‹¤.\n"
            f" 1. [Facial Regions ë¶„ì„ ëŒ€ìƒ]: ë¶„ì„ì— ì‚¬ìš©ëœ ì–¼êµ´ ë¶€ìœ„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {', '.join(summary['facial_region'])}. ('None'ì€ REALë¡œ íŒë‹¨ë˜ì–´ Grad-CAMì´ ê·¸ë ¤ì§€ì§€ ì•Šì€ ê²½ìš°ì…ë‹ˆë‹¤.)\n\n"
            f" 2. [1ìˆœìœ„ í™œì„±í™” íšŸìˆ˜ (First Detection Count)]: ê° ë¶€ìœ„ê°€ Grad-CAMì—ì„œ 1ìˆœìœ„ë¡œ ê°€ì¥ í™œì„±í™”ëœ í”„ë ˆì„ ìˆ˜ì…ë‹ˆë‹¤.\n"
            f" {summary['first_detection_count']}\n\n"
            f" 3. [2ìˆœìœ„ í™œì„±í™” íšŸìˆ˜ (Second Detection Count)]: ê° ë¶€ìœ„ê°€ 2ìˆœìœ„ë¡œ í™œì„±í™”ëœ í”„ë ˆì„ ìˆ˜ì…ë‹ˆë‹¤.\n"
            f" {summary['second_detection_count']}\n\n"
            f" 4. [1ìˆœìœ„ í™œì„±í™” ë¹„ìœ¨ (First Detection Rate)]: ì „ì²´ í”„ë ˆì„ ì¤‘ ê° ë¶€ìœ„ê°€ 1ìˆœìœ„ë¡œ ì„ íƒëœ ë¹„ìœ¨ì…ë‹ˆë‹¤ (% ë‹¨ìœ„).\n"
            f" {summary['first_detection_rate']}\n\n"
            f" 5. [2ìˆœìœ„ í™œì„±í™” ë¹„ìœ¨ (Second Detection Rate)]: ì „ì²´ í”„ë ˆì„ ì¤‘ ê° ë¶€ìœ„ê°€ 2ìˆœìœ„ë¡œ ì„ íƒëœ ë¹„ìœ¨ì…ë‹ˆë‹¤ (% ë‹¨ìœ„).\n"
            f" {summary['second_detection_rate']}\n\n"
            
        )



        # FAKE íŒë‹¨ì— ëŒ€í•œ ë¶„ì„
        if summary['binary_pred'] == 'FAKE':
            prompt += (
                f" 6. [Raw Detection Probability]: ê° ë¶€ìœ„ì— ëŒ€í•´ Grad-CAMì˜ ì´ í™œì„± ê¸°ì—¬ë„ë¥¼ raw ì ìˆ˜ë¡œ ë‚˜íƒ€ë‚¸ ê°’ì…ë‹ˆë‹¤. ê° í”„ë ˆì„ë³„ì—ì„œ (Fakeì¼ í™•ë¥  Ã— ë¶€ìœ„ë³„ ROI í‰ê·  í™œì„±ë„)ë¥¼ ê³„ì‚°í•˜ì—¬, ë¶€ìœ„ë³„ë¡œ ëª¨ë‘ ë”í•˜ì—¬ êµ¬í•©ë‹ˆë‹¤. \n"
                f" {summary['raw_detection_probability']}\n\n"
                f" 7. [Normalized Detection Contribution (%)]: ìœ„ì˜ raw ê°’ë“¤ì„ ì „ì²´ í•©ìœ¼ë¡œ ë‚˜ëˆ  ì •ê·œí™”í•œ ê°’ì…ë‹ˆë‹¤. ê° ë¶€ìœ„ê°€ ì „ì²´ íŒë‹¨ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ ìƒëŒ€ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. \n"
                f" {summary['detection_probability']}\n\n"
            )
            prompt += (
                "ëª¨ë¸ ê²°ê³¼ê°€ FAKEë¡œ íŒë‹¨ëœ ê²½ìš°, ë”¥í˜ì´í¬ ê¸°ë²• ë¶„ë¥˜ ê²°ê³¼ì™€ ìœ„ì˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì–´ë–¤ ì–¼êµ´ ë¶€ìœ„ê°€ ì–´ë–»ê²Œ ì‘ìš©í–ˆëŠ”ì§€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ê³ , "
                "ì¤‘ìš”ë„ê°€ ë†’ì€ ë¶„ì„ ë‚´ìš©ì„ ì„ ë³„í•˜ì—¬ ë…ìê°€ ë‚©ë“í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•´ ì£¼ì„¸ìš”.\n"
                "ëª¨ë¸ ê²°ê³¼ê°€ FAKEë¡œ íŒë‹¨ëœ ê²½ìš°ì—ëŠ” Grad-CAM ë¶„ì„ì´ ë§¤ìš° ì¤‘ìš”í•˜ë©°, íŠ¹ì • ì–¼êµ´ ë¶€ìœ„ë“¤ì´ ì–´ë–»ê²Œ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.\n"
        )

        # REAL íŒë‹¨ì— ëŒ€í•œ ë¶„ì„
        elif summary['binary_pred'] == 'REAL':
            prompt += (
            "ëª¨ë¸ ê²°ê³¼ê°€ REALë¡œ íŒë‹¨ëœ ê²½ìš°, Noneì˜ detection_count ê°’ì„ ì°¸ê³ í•˜ì—¬ ë¶„ì„í•˜ê³ , ì¤‘ìš”ë„ê°€ ë†’ì€ ë¶„ì„ ë‚´ìš©ì„ ì„ ë³„í•˜ì—¬ ë…ìê°€ ë‚©ë“í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•´ ì£¼ì„¸ìš”.\n"
            "REALë¡œ íŒë‹¨ëœ ê²½ìš°, ì–¼êµ´ ë¶€ìœ„ë³„ ê°ì§€ íšŸìˆ˜ê°€ ì ê³  'None'ì˜ íšŸìˆ˜ê°€ ëŒ€ë¶€ë¶„ì´ê¸° ë•Œë¬¸ì— REALë¡œ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. "
            "Grad-CAMì´ ê·¸ë ¤ì§€ì§€ ì•Šê±°ë‚˜, í™œì„±í™”ê°€ ì ì–´, ëª¨ë¸ì´ ì‹¤ì œ ì˜ìƒì´ë¼ê³  ì˜ˆì¸¡í•œ ì´ìœ ë¥¼ ëª…í™•íˆ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n"
        )


        return prompt

    def query_model(prompt):
        url = "http://localhost:11434/api/generate"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "llama3",
            "prompt": prompt
        }

        response = requests.post(url, headers=headers, data=json.dumps(data))
        print("ì‘ë‹µ ì½”ë“œ : ",response.status_code)
        if response.status_code == 200:
            print("ì‘ë‹µ ë‚´ìš©:")
            # ì‘ë‹µ ë‚´ìš©ì´ ì—¬ëŸ¬ ê°œë¡œ ìª¼ê°œì ¸ì„œ ì˜¤ëŠ” ê²½ìš° í•©ì¹˜ê¸°
            combined_response = ''
            
            # ì„œë²„ ì‘ë‹µì´ ì—¬ëŸ¬ ì¤„ë¡œ ì˜¨ë‹¤ë©´, ê° ì¤„ì„ ì²˜ë¦¬í•´ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
            for line in response.text.splitlines():
                try:
                    # ê° ì¤„ì„ JSONìœ¼ë¡œ íŒŒì‹±
                    json_line = json.loads(line)
                    # 'response' í‚¤ì˜ ê°’ë§Œ í•©ì¹¨
                    combined_response += json_line['response']
                except json.JSONDecodeError:
                    continue  # ì˜ëª»ëœ ì¤„ì€ ë¬´ì‹œ

            # í•©ì¹œ ì‘ë‹µ ì¶œë ¥
            print(combined_response)
            return combined_response
        else:
            print("ìš”ì²­ ì‹¤íŒ¨:", response.status_code)

    print("í”„ë¡¬í”„íŠ¸ ë‚´ìš© : ",format_prompt(roi_analyze_result))
    response_txt=query_model(format_prompt(roi_analyze_result))
    return response_txt, table_data

