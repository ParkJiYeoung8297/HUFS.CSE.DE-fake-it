import cv2
import torch
import torchvision.transforms as T
import numpy as np
import torch.nn.functional as F
from torchvision import models
from torch import nn
import os
import glob
import pandas as pd
import face_alignment
from tqdm import tqdm
from django.conf import settings
from pathlib import Path
import subprocess
import cv2
import json
import requests
from .model import Model

checkpoint_path=Path(__file__).resolve().parent

# ‚úÖ Set the device to MPS(for Mac) if available, otherwise fallback to CUDA or CPU
device = torch.device("mps") if torch.backends.mps.is_available() else (
torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
)
print(f"Using device: {device}")




# ‚úÖ Grad-CAM computation for binary classification
def compute_gradcam_binary(model, input_tensor, target_class=0,device = torch.device("mps")):
    fmap = None
    grad = None

    def fw_hook(module, inp, out):
        nonlocal fmap
        fmap = out.detach()

    def bw_hook(module, grad_in, grad_out):
        nonlocal grad
        grad = grad_out[0].detach()

    last_layer = model.model[-1]
    f = last_layer.register_forward_hook(fw_hook)
    b = last_layer.register_backward_hook(bw_hook)

    input_tensor = input_tensor.to(device).unsqueeze(0).unsqueeze(0).requires_grad_(True)
    _, binary_output, method_output = model(input_tensor)

    # Get the probability of the target class
    prob = F.softmax(binary_output, dim=1)[0, target_class].item()

    # Predict binary and method classes
    binary_pred = torch.argmax(binary_output, dim=1).item()   # 0: fake, 1: real
    method_pred = torch.argmax(method_output, dim=1).item()   # 0: original, 1~6: fake methods, 7: others

    # üî¥ Condition 1: If the prediction is real and the method is original, skip CAM computation / Ï°∞Í±¥ 1: real(1) + original(0) ‚Üí CAM X
    if binary_pred == 1 and method_pred==0:
        cam = np.zeros((input_tensor.shape[-2], input_tensor.shape[-1]))
        f.remove()
        b.remove()
        return cam,prob,binary_pred, method_pred

    # Grad-CAM for fake class (target_class = 0)
    target_class = 0
    model.zero_grad()
    binary_output[0, target_class].backward()

    # Compute Grad-CAM
    weights = grad.mean(dim=[2, 3], keepdim=True)
    cam = (weights * fmap).sum(dim=1, keepdim=True)
    cam = F.relu(cam).squeeze().cpu().numpy()

    # üîµ Condition 2: If the prediction is fake and the method is not original, enhance CAM / Ï°∞Í±¥ 2: fake (0) + method (1~7) (‚â† 0) ‚Üí CAM ‚Üë
    if binary_pred == 0 and method_pred != 0:
        cam *=1.5

    # Normalize and resize the CAM
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
    cam = cv2.resize(cam, (input_tensor.shape[-1], input_tensor.shape[-2]))



    f.remove()
    b.remove()
    return cam, prob, binary_pred, method_pred

def get_bbox(pts):
    x, y = pts[:,0], pts[:,1]
    return int(x.min()), int(y.min()), int(x.max()), int(y.max())

def roi_activation(cam, bbox):
    x1, y1, x2, y2 = bbox
    patch = cam[y1:y2, x1:x2]
    mean_val = float(patch.mean())

    if np.isnan(mean_val):
        return -1
    return mean_val

def analyze_roi_activation(video_dir,file_name, result,model):

    fa = face_alignment.FaceAlignment(
        face_alignment.LandmarksType.TWO_D,
        device=str(device)
    )

    os.makedirs(video_dir, exist_ok=True)

    transform = T.Compose([
        T.ToTensor(),
        T.Resize((224, 224)),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])

    roi_result = []
    roi_analyze_result={}

    # video_paths = glob.glob(os.path.join(video_path, '*.mp4'))

#   for video_path in tqdm(video_paths):

    facial_region=['jawline', 'left_eye', 'right_eye', 'left_eye_brow', 'right_eye_brow', 'nose', 'mouth','None']
    first_detection_count = {key: 0 for key in facial_region}
    second_detection_count = {key: 0 for key in facial_region}
    detection_probabillity={key: 0.0 for key in facial_region}


    video_path_file_name=os.path.join(video_dir,file_name)
    cap = cv2.VideoCapture(video_path_file_name)
    frame_idx = 0
    # video_name = os.path.splitext(os.path.basename(video_path))[0]


    # ‚ñ∂Ô∏è ÎπÑÎîîÏò§ Ï†ÄÏû• ÏÑ§Ï†ï
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Î∞ïÏä§Îßå Í∑∏Î¶∞ ÏòÅÏÉÅ
    video_writer_box = cv2.VideoWriter(
        os.path.join(video_dir, "output_box_on_original.mp4"),
        fourcc, fps, (width, height)
    )

    # Grad-CAM + Î∞ïÏä§ ÏòÅÏÉÅ
    video_writer_cam = cv2.VideoWriter(
        os.path.join(video_dir, f"grad_cam_on_original.mp4"),
        fourcc, fps, (width, height)
    )

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        landmarks = fa.get_landmarks(rgb)
        if not landmarks:
            frame_idx += 1
            continue
        lm = landmarks[0]

        # ROI BBoxes
        bbox_map = {
            'jawline': get_bbox(lm[0:17]),
            'left_eye': get_bbox(lm[36:42]),
            'right_eye': get_bbox(lm[42:48]),
            'left_eye_brow': get_bbox(lm[17:22]),
            'right_eye_brow': get_bbox(lm[22:27]),
            'nose': get_bbox(lm[27:36]),
            'mouth': get_bbox(lm[48:68]),
        }

        # Grad-CAM
        img = transform(rgb).to(device)
        cam, cam_score, binary_pred, method_pred = compute_gradcam_binary(model, img)
        cam = cv2.resize(cam, (frame.shape[1], frame.shape[0]))

        scores = {region: roi_activation(cam, box) for region, box in bbox_map.items()}
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        first_activated_region = sorted_scores[0][0]
        second_activated_region = sorted_scores[1][0]

        f_x1, f_y1, f_x2, f_y2 = bbox_map[first_activated_region]
        s_x1, s_y1, s_x2, s_y2 = bbox_map[second_activated_region]

        # 1Ô∏è‚É£ Grad-CAM ÌûàÌä∏Îßµ Ïò§Î≤ÑÎ†àÏù¥
        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
        overlay = cv2.addWeighted(frame, 0.6, heatmap, 0.4, 0)

        # 2Ô∏è‚É£ ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Î≥µÏÇ¨Ìï¥ÏÑú Î∞ïÏä§Ïö© Ï§ÄÎπÑ
        frame_with_box = frame.copy()
        overlay_with_box = overlay.copy()

        # 3Ô∏è‚É£ Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
        if scores[first_activated_region] > 0:
            cv2.rectangle(frame_with_box, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)
            cv2.rectangle(overlay_with_box, (f_x1, f_y1), (f_x2, f_y2), (0, 255, 0), 2)
        else:
            first_activated_region="None"

        if scores[second_activated_region] > 0:
            cv2.rectangle(frame_with_box, (s_x1, s_y1), (s_x2, s_y2), (255, 0, 0), 2)
            cv2.rectangle(overlay_with_box, (s_x1, s_y1), (s_x2, s_y2), (255, 0, 0), 2)
        else:
            second_activated_region="None"

    #   # 4Ô∏è‚É£ ÌååÏùº Ï†ÄÏû•
        file_id = f"{file_name}_frame{frame_idx:04d}"
    #   cv2.imwrite(os.path.join(output_dir_box, f"{file_id}_roi.jpg"), frame_with_box)
    #   cv2.imwrite(os.path.join(output_dir_box, f"{file_id}_gradcam.jpg"), overlay_with_box)

    #  4Ô∏è‚É£ ÏòÅÏÉÅ ÌååÏùº Ï†ÄÏû•
        video_writer_box.write(frame_with_box)
        video_writer_cam.write(overlay_with_box)

        first_detection_count[first_activated_region]+=1
        second_detection_count[second_activated_region]+=1
        for key in facial_region:
            if key!="None" and scores[key]!=-1:
                detection_probabillity[key]+=cam_score * scores[key]

        roi_result.append({
            'file_name': file_id,
            'cam_score': cam_score,
            'first_activate_region': first_activated_region,
            'second_activate_region': second_activated_region,
            'f_x1': f_x1, 'f_y1': f_y1, 'f_x2': f_x2, 'f_y2': f_y2,
            's_x1': s_x1, 's_y1': s_y1, 's_x2': s_x2, 's_y2': s_y2,
            **scores,
        })

        frame_idx += 1

    cap.release()
    video_writer_box.release()
    video_writer_cam.release()

    # Printing all the dictionaries
    first_detection_rate = {key: round((value / frame_idx)*100, 2) for key, value in first_detection_count.items()}
    second_detection_rate = {key: round((value / frame_idx)*100, 2) for key, value in second_detection_count.items()}

    # üìå Note: A high proportion of 'None' may inflate the relative Contribution (%) and should be interpreted with caution.
    raw_detection_probabillity= {key: round(value, 4) for key, value in detection_probabillity.items()}
    probabillity_total = sum(detection_probabillity.values())
    detection_probabillity= {key: round((value/probabillity_total)*100, 2) for key, value in detection_probabillity.items()}

    # print("Video name:", file_name)
    # print("Facial Region:", facial_region)
    # print("First Detection Count:", first_detection_count)
    # print("Second Detection Count:", second_detection_count)
    # print("First Detection Rate:", first_detection_rate)
    # print("Second Detection Rate:", second_detection_rate)
    # print("Raw_Detection Probability:", raw_detection_probabillity)
    # print("Detection Probability:", detection_probabillity)

    roi_analyze_result = {
    "video_name": file_name,
    "binary_pred":result["Prediction"], 
    "cam_score":result["Probability"],
    "method_pred":result["Method"],
    "facial_region": facial_region,
    "first_detection_count": first_detection_count,
    "second_detection_count": second_detection_count,
    "first_detection_rate": first_detection_rate,
    "second_detection_rate": second_detection_rate,
    "raw_detection_probability": raw_detection_probabillity,
    "detection_probability": detection_probabillity
    }

    # ÌÖåÏù¥Î∏î ÎßåÎì§Í∏∞
    table_data = []
    for region in facial_region:
        row = {
            "region": region,
            "first_count": f"{first_detection_count.get(region, '0')} ({first_detection_rate.get(region, '0.00')}%)",
            "second_count": f"{second_detection_count.get(region, '0')} ({second_detection_rate.get(region, '0.00')}%)",
            "confidence": "-" if detection_probabillity.get(region) is None else f"{detection_probabillity.get(region, '0.00')}%"
        }
        table_data.append(row)

    # ÎßàÏßÄÎßâ Ï¥ùÌï© Ìñâ Ï∂îÍ∞Ä
    table_data.append({
        "region": "total",
        "first_count": sum(int(v) for v in first_detection_count.values()),
        "second_count": sum(int(v) for v in second_detection_count.values()),
        "confidence": "100.00%"
    })


    return roi_analyze_result,table_data



def all_calculate_roi_scores(video_path,file_name,result,checkpoint_name='checkpoint_v35',selected_model='EfficientNet-b0'):
    print("Ïó¨Í∏∞Ïöî1")
    # Î™®Îç∏ Íµ¨Ï°∞Î•º Ï†ïÏùò
    model = Model(num_binary_classes=2, num_method_classes=7, model_name=selected_model).to(device)
    model.load_state_dict(torch.load(f'{checkpoint_path}/{checkpoint_name}.pt', map_location=device))
    model.eval()

    roi_analyze_result,table_data=analyze_roi_activation(video_path,file_name,result, model)

    # Ïù¥ÎØ∏ Ï†ÄÏû•Îêú ÏõêÎ≥∏ ÏòÅÏÉÅ Í≤ΩÎ°ú
    grad_video_path_original = os.path.join(video_path, 'grad_cam_on_original.mp4')

    # Î≥ÄÌôò ÌõÑ Ï†ÄÏû•Ìï† Í≤ΩÎ°ú ÏÑ§Ï†ï
    output_filename = "converted_grad_cam_on_original.mp4"
    output_path = os.path.join(video_path, output_filename)

    # ffmpegÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î≥ÄÌôò ÏàòÌñâ
    try:
        subprocess.run([
            'ffmpeg', '-i', grad_video_path_original,
            '-vcodec', 'libx264',
            '-acodec', 'aac',
            '-strict', 'experimental',
            '-y',  # ÎçÆÏñ¥Ïì∞Í∏∞
            output_path
        ], check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        print(f"‚úÖ Video converted: {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error occurred during conversion: {e}")

    # Ïù¥ÎØ∏ Ï†ÄÏû•Îêú ÏõêÎ≥∏ ÏòÅÏÉÅ Í≤ΩÎ°ú
    grad_video_path_original = os.path.join(video_path, 'output_box_on_original.mp4')

    # Î≥ÄÌôò ÌõÑ Ï†ÄÏû•Ìï† Í≤ΩÎ°ú ÏÑ§Ï†ï
    output_filename = "converted_output_box_on_original.mp4"
    output_path = os.path.join(video_path, output_filename)

    # ffmpegÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î≥ÄÌôò ÏàòÌñâ
    try:
        subprocess.run([
            'ffmpeg', '-i', grad_video_path_original,
            '-vcodec', 'libx264',
            '-acodec', 'aac',
            '-strict', 'experimental',
            '-y',  # ÎçÆÏñ¥Ïì∞Í∏∞
            output_path
        ], check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        print(f"‚úÖ Video converted: {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error occurred during conversion: {e}")


#Ïó¨Í∏∞Î∂ÄÌÑ∞
    def format_prompt(summary):
        print(f"summary: {summary}")
        prompt = (
            f" ÏïÑÎûòÏóê Ï†úÍ≥µÎêú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Î™®Îç∏Ïù¥ Ìï¥Îãπ ÏòÅÏÉÅÏùÑ REAL ÎòêÎäî FAKEÎ°ú ÌåêÎã®Ìïú Í∑ºÍ±∞Î•º ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî."
            f"ÏùëÎãµÏùÄ Î∂ÑÏÑù ÎÇ¥Ïö©Îßå Ìè¨Ìï®ÌïòÍ≥†, Îã§Ïùå ÌòïÏãùÏ≤òÎüº Î≤àÌò∏Î•º Î∂ôÏó¨Ï£ºÏÑ∏Ïöî:\n"
            f"1. ...\n2. ...\n3. ...\n"
            f"Î™®Îç∏ ÏòàÏ∏° Í≤∞Í≥º:Ïù¥ Î™®Îç∏ÏùÄ REAL/FAKE ÌåêÎã®ÏóêÏÑú {summary['binary_pred']}Î°ú ÏòàÏ∏°ÌñàÏúºÎ©∞, ÌôïÎ•†ÏùÄ {summary['cam_score']}ÏûÖÎãàÎã§. Îî•ÌéòÏù¥ÌÅ¨ Í∏∞Î≤ï Î∂ÑÎ•ò Í≤∞Í≥º: {summary['method_pred']}"
            f"Ï∞∏Í≥† : originalÏùÄ ÏúÑÏ°∞ ÌùîÏ†ÅÏù¥ ÏóÜÎäî ÏõêÎ≥∏ ÏòÅÏÉÅ, othersÎäî FaceForensics++Ïùò 5Í∞ÄÏßÄ Í∏∞Î≤ï Ïô∏Ïùò ÏúÑÏ°∞ Î∞©ÏãùÏûÖÎãàÎã§."
            f"ÏïÑÎûòÎäî ÏòÅÏÉÅ {summary['video_name']}Ïóê ÎåÄÌïú Grad-CAM Í∏∞Î∞ò ROI ÌôúÏÑ±ÎèÑ Î∂ÑÏÑù ÏöîÏïΩÏûÖÎãàÎã§."

            f"Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏÑ§Î™Ö (Î™®Îì† Í∞íÏùÄ ÏòÅÏÉÅ Ï†ÑÏ≤¥ ÌîÑÎ†àÏûÑÏùÑ ÌÜµÌï©Ìïú ÌÜµÍ≥Ñ Í∏∞Î∞òÏûÖÎãàÎã§):\n"
            f" - Í∞ÄÏßúÏóê ÏòÅÌñ•ÏùÑ Ï£ºÎäî Î∂ÄÎ∂ÑÏùÑ grad-camÏúºÎ°ú ÏãúÍ∞ÅÌôîÌïòÏó¨, ÏòÅÏÉÅ ÎÇ¥Ïóê Î™®Îì† ÌîÑÎ†àÏûÑÏùÑ ÌÜµÌï©Ìïú Í∞íÏûÖÎãàÎã§.\n"
            f" 1. [Facial Regions Î∂ÑÏÑù ÎåÄÏÉÅ]: Î∂ÑÏÑùÏóê ÏÇ¨Ïö©Îêú ÏñºÍµ¥ Î∂ÄÏúÑÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§: {', '.join(summary['facial_region'])}. ('None'ÏùÄ REALÎ°ú ÌåêÎã®ÎêòÏñ¥ Grad-CAMÏù¥ Í∑∏Î†§ÏßÄÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ÏûÖÎãàÎã§.)\n\n"
            f" 2. [1ÏàúÏúÑ ÌôúÏÑ±Ìôî ÌöüÏàò (First Detection Count)]: Í∞Å Î∂ÄÏúÑÍ∞Ä Grad-CAMÏóêÏÑú 1ÏàúÏúÑÎ°ú Í∞ÄÏû• ÌôúÏÑ±ÌôîÎêú ÌîÑÎ†àÏûÑ ÏàòÏûÖÎãàÎã§.\n"
            f" {summary['first_detection_count']}\n\n"
            f" 3. [2ÏàúÏúÑ ÌôúÏÑ±Ìôî ÌöüÏàò (Second Detection Count)]: Í∞Å Î∂ÄÏúÑÍ∞Ä 2ÏàúÏúÑÎ°ú ÌôúÏÑ±ÌôîÎêú ÌîÑÎ†àÏûÑ ÏàòÏûÖÎãàÎã§.\n"
            f" {summary['second_detection_count']}\n\n"
            f" 4. [1ÏàúÏúÑ ÌôúÏÑ±Ìôî ÎπÑÏú® (First Detection Rate)]: Ï†ÑÏ≤¥ ÌîÑÎ†àÏûÑ Ï§ë Í∞Å Î∂ÄÏúÑÍ∞Ä 1ÏàúÏúÑÎ°ú ÏÑ†ÌÉùÎêú ÎπÑÏú®ÏûÖÎãàÎã§ (% Îã®ÏúÑ).\n"
            f" {summary['first_detection_rate']}\n\n"
            f" 5. [2ÏàúÏúÑ ÌôúÏÑ±Ìôî ÎπÑÏú® (Second Detection Rate)]: Ï†ÑÏ≤¥ ÌîÑÎ†àÏûÑ Ï§ë Í∞Å Î∂ÄÏúÑÍ∞Ä 2ÏàúÏúÑÎ°ú ÏÑ†ÌÉùÎêú ÎπÑÏú®ÏûÖÎãàÎã§ (% Îã®ÏúÑ).\n"
            f" {summary['second_detection_rate']}\n\n"
            
        )



        # FAKE ÌåêÎã®Ïóê ÎåÄÌïú Î∂ÑÏÑù
        if summary['binary_pred'] == 'FAKE':
            prompt += (
                f" 6. [Raw Detection Probability]: Í∞Å Î∂ÄÏúÑÏóê ÎåÄÌï¥ Grad-CAMÏùò Ï¥ù ÌôúÏÑ± Í∏∞Ïó¨ÎèÑÎ•º raw Ï†êÏàòÎ°ú ÎÇòÌÉÄÎÇ∏ Í∞íÏûÖÎãàÎã§. Í∞Å ÌîÑÎ†àÏûÑÎ≥ÑÏóêÏÑú (FakeÏùº ÌôïÎ•† √ó Î∂ÄÏúÑÎ≥Ñ ROI ÌèâÍ∑† ÌôúÏÑ±ÎèÑ)Î•º Í≥ÑÏÇ∞ÌïòÏó¨, Î∂ÄÏúÑÎ≥ÑÎ°ú Î™®Îëê ÎçîÌïòÏó¨ Íµ¨Ìï©ÎãàÎã§. \n"
                f" {summary['raw_detection_probability']}\n\n"
                f" 7. [Normalized Detection Contribution (%)]: ÏúÑÏùò raw Í∞íÎì§ÏùÑ Ï†ÑÏ≤¥ Ìï©ÏúºÎ°ú ÎÇòÎà† Ï†ïÍ∑úÌôîÌïú Í∞íÏûÖÎãàÎã§. Í∞Å Î∂ÄÏúÑÍ∞Ä Ï†ÑÏ≤¥ ÌåêÎã®Ïóê ÏñºÎßàÎÇò Í∏∞Ïó¨ÌñàÎäîÏßÄ ÏÉÅÎåÄÏ†ÅÏúºÎ°ú Î≥¥Ïó¨Ï§çÎãàÎã§. \n"
                f" {summary['detection_probability']}\n\n"
            )
            prompt += (
                "Î™®Îç∏ Í≤∞Í≥ºÍ∞Ä FAKEÎ°ú ÌåêÎã®Îêú Í≤ΩÏö∞, Îî•ÌéòÏù¥ÌÅ¨ Í∏∞Î≤ï Î∂ÑÎ•ò Í≤∞Í≥ºÏôÄ ÏúÑÏùò Ï†ïÎ≥¥Î•º Ï∞∏Í≥†ÌïòÏó¨ Ïñ¥Îñ§ ÏñºÍµ¥ Î∂ÄÏúÑÍ∞Ä Ïñ¥ÎñªÍ≤å ÏûëÏö©ÌñàÎäîÏßÄÎ•º Ï§ëÏã¨ÏúºÎ°ú Î∂ÑÏÑùÌïòÍ≥†, "
                "Ï§ëÏöîÎèÑÍ∞Ä ÎÜíÏùÄ Î∂ÑÏÑù ÎÇ¥Ïö©ÏùÑ ÏÑ†Î≥ÑÌïòÏó¨ ÎèÖÏûêÍ∞Ä ÎÇ©ÎìùÌï† Ïàò ÏûàÎèÑÎ°ù Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏÑúÏà†Ìï¥ Ï£ºÏÑ∏Ïöî.\n"
                "Î™®Îç∏ Í≤∞Í≥ºÍ∞Ä FAKEÎ°ú ÌåêÎã®Îêú Í≤ΩÏö∞ÏóêÎäî Grad-CAM Î∂ÑÏÑùÏù¥ Îß§Ïö∞ Ï§ëÏöîÌïòÎ©∞, ÌäπÏ†ï ÏñºÍµ¥ Î∂ÄÏúÑÎì§Ïù¥ Ïñ¥ÎñªÍ≤å ÌôúÏÑ±ÌôîÎêòÏóàÎäîÏßÄÎ•º Î∂ÑÏÑùÌï¥ Ï£ºÏÑ∏Ïöî.\n"
        )

        # REAL ÌåêÎã®Ïóê ÎåÄÌïú Î∂ÑÏÑù
        elif summary['binary_pred'] == 'REAL':
            prompt += (
            "Î™®Îç∏ Í≤∞Í≥ºÍ∞Ä REALÎ°ú ÌåêÎã®Îêú Í≤ΩÏö∞, NoneÏùò detection_count Í∞íÏùÑ Ï∞∏Í≥†ÌïòÏó¨ Î∂ÑÏÑùÌïòÍ≥†, Ï§ëÏöîÎèÑÍ∞Ä ÎÜíÏùÄ Î∂ÑÏÑù ÎÇ¥Ïö©ÏùÑ ÏÑ†Î≥ÑÌïòÏó¨ ÎèÖÏûêÍ∞Ä ÎÇ©ÎìùÌï† Ïàò ÏûàÎèÑÎ°ù Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏÑúÏà†Ìï¥ Ï£ºÏÑ∏Ïöî.\n"
            "REALÎ°ú ÌåêÎã®Îêú Í≤ΩÏö∞, ÏñºÍµ¥ Î∂ÄÏúÑÎ≥Ñ Í∞êÏßÄ ÌöüÏàòÍ∞Ä Ï†ÅÍ≥† 'None'Ïùò ÌöüÏàòÍ∞Ä ÎåÄÎ∂ÄÎ∂ÑÏù¥Í∏∞ ÎïåÎ¨∏Ïóê REALÎ°ú ÌåêÎã®ÎêòÏóàÏäµÎãàÎã§. "
            "Grad-CAMÏù¥ Í∑∏Î†§ÏßÄÏßÄ ÏïäÍ±∞ÎÇò, ÌôúÏÑ±ÌôîÍ∞Ä Ï†ÅÏñ¥, Î™®Îç∏Ïù¥ Ïã§Ï†ú ÏòÅÏÉÅÏù¥ÎùºÍ≥† ÏòàÏ∏°Ìïú Ïù¥Ïú†Î•º Î™ÖÌôïÌûà Ìï† Ïàò ÏûàÏäµÎãàÎã§. \n"
        )


        return prompt

    def query_model(prompt):
        url = "http://localhost:11434/api/generate"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "llama3",
            "prompt": prompt
        }

        response = requests.post(url, headers=headers, data=json.dumps(data))
        print("ÏùëÎãµ ÏΩîÎìú : ",response.status_code)
        if response.status_code == 200:
            print("ÏùëÎãµ ÎÇ¥Ïö©:")
            # ÏùëÎãµ ÎÇ¥Ïö©Ïù¥ Ïó¨Îü¨ Í∞úÎ°ú Ï™ºÍ∞úÏ†∏ÏÑú Ïò§Îäî Í≤ΩÏö∞ Ìï©ÏπòÍ∏∞
            combined_response = ''
            
            # ÏÑúÎ≤Ñ ÏùëÎãµÏù¥ Ïó¨Îü¨ Ï§ÑÎ°ú Ïò®Îã§Î©¥, Í∞Å Ï§ÑÏùÑ Ï≤òÎ¶¨Ìï¥ÏÑú ÌïòÎÇòÏùò Î¨∏ÏûêÏó¥Î°ú Ìï©Ïπ®
            for line in response.text.splitlines():
                try:
                    # Í∞Å Ï§ÑÏùÑ JSONÏúºÎ°ú ÌååÏã±
                    json_line = json.loads(line)
                    # 'response' ÌÇ§Ïùò Í∞íÎßå Ìï©Ïπ®
                    combined_response += json_line['response']
                except json.JSONDecodeError:
                    continue  # ÏûòÎ™ªÎêú Ï§ÑÏùÄ Î¨¥Ïãú

            # Ìï©Ïπú ÏùëÎãµ Ï∂úÎ†•
            print(combined_response)
            return combined_response
        else:
            print("ÏöîÏ≤≠ Ïã§Ìå®:", response.status_code)

    print("ÌîÑÎ°¨ÌîÑÌä∏ ÎÇ¥Ïö© : ",format_prompt(roi_analyze_result))
    response_txt=query_model(format_prompt(roi_analyze_result))
    return response_txt, table_data

